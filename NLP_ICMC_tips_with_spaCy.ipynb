{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_ICMC_tips_with_spaCy.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNDfxKVRGusMxMwheo5Vq+b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rogerio-mack/work/blob/main/NLP_ICMC_tips_with_spaCy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwIMMugUF2SS"
      },
      "source": [
        "https://cursosextensao.usp.br/course/view.php?id=2721"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zy9M1Nq3hCPS"
      },
      "source": [
        "# Create e Write a file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Lp2u-Inf0MB"
      },
      "source": [
        "outfile = open('/content/output.txt','w')\n",
        "for texto in ['Hello!\\n','Aqui estamos... \\n']:\n",
        "  outfile.write(texto)\n",
        "\n",
        "outfile.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJj3ESHAhJ2r"
      },
      "source": [
        "# Write *append* in a file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXddnjzYg3xy"
      },
      "source": [
        "outfile = open('/content/output.txt','a')\n",
        "outfile.write('Até breve!')\n",
        "\n",
        "outfile.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TsI-QDWpTCy"
      },
      "source": [
        "# Create e Write a file 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEMlJlyWo9tu"
      },
      "source": [
        "poema = \"\"\"Ou se tem chuva e não se tem sol,\n",
        "ou se tem sol e não se tem chuva!\n",
        "Ou se calça a luva e não se põe o anel,\n",
        "ou se põe o anel e não se calça a luva!\n",
        "Quem sobe nos ares não fica no chão,\n",
        "quem fica no chão não sobe nos ares.\n",
        "É uma grande pena que não se possa\n",
        "estar ao mesmo tempo nos dois lugares!\n",
        "Ou guardo o dinheiro e não compro o doce,\n",
        "ou compro o doce e gasto o dinheiro.\n",
        "Ou isto ou aquilo: ou isto ou aquilo…\n",
        "e vivo escolhendo o dia inteiro!\n",
        "Não sei se brinco, não sei se estudo,\n",
        "se saio correndo ou fico tranquilo.\n",
        "Mas não consegui entender ainda\n",
        "qual é melhor: se é isto ou aquilo.\n",
        "\n",
        "(Ou Isto ou Aquilo – Cecília Meireles)\"\"\"\n",
        "\n",
        "outfile = open('/content/output.txt','w')\n",
        "for texto in poema:\n",
        "  outfile.write(texto)\n",
        "\n",
        "outfile.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwoE65fQhPXk"
      },
      "source": [
        "# Read a file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMos4wh-gE6U",
        "outputId": "63cc9053-68a4-45a0-b0fc-7cc66817c5da"
      },
      "source": [
        "infile = open('/content/output.txt','r')\n",
        "print(infile.read())\n",
        "infile.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ou se tem chuva e não se tem sol,\n",
            "ou se tem sol e não se tem chuva!\n",
            "Ou se calça a luva e não se põe o anel,\n",
            "ou se põe o anel e não se calça a luva!\n",
            "Quem sobe nos ares não fica no chão,\n",
            "quem fica no chão não sobe nos ares.\n",
            "É uma grande pena que não se possa\n",
            "estar ao mesmo tempo nos dois lugares!\n",
            "Ou guardo o dinheiro e não compro o doce,\n",
            "ou compro o doce e gasto o dinheiro.\n",
            "Ou isto ou aquilo: ou isto ou aquilo…\n",
            "e vivo escolhendo o dia inteiro!\n",
            "Não sei se brinco, não sei se estudo,\n",
            "se saio correndo ou fico tranquilo.\n",
            "Mas não consegui entender ainda\n",
            "qual é melhor: se é isto ou aquilo.\n",
            "\n",
            "(Ou Isto ou Aquilo – Cecília Meireles)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cp1Ft2vqiu1C"
      },
      "source": [
        "# Read a file *line by line*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMbkXP49i1gK",
        "outputId": "9b4b1742-f7fa-49de-f009-de373d6b16c8"
      },
      "source": [
        "infile = open('/content/output.txt','r') \n",
        "for line in infile:\n",
        "  print(line)\n",
        "\n",
        "infile.close()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ou se tem chuva e não se tem sol,\n",
            "\n",
            "ou se tem sol e não se tem chuva!\n",
            "\n",
            "Ou se calça a luva e não se põe o anel,\n",
            "\n",
            "ou se põe o anel e não se calça a luva!\n",
            "\n",
            "Quem sobe nos ares não fica no chão,\n",
            "\n",
            "quem fica no chão não sobe nos ares.\n",
            "\n",
            "É uma grande pena que não se possa\n",
            "\n",
            "estar ao mesmo tempo nos dois lugares!\n",
            "\n",
            "Ou guardo o dinheiro e não compro o doce,\n",
            "\n",
            "ou compro o doce e gasto o dinheiro.\n",
            "\n",
            "Ou isto ou aquilo: ou isto ou aquilo…\n",
            "\n",
            "e vivo escolhendo o dia inteiro!\n",
            "\n",
            "Não sei se brinco, não sei se estudo,\n",
            "\n",
            "se saio correndo ou fico tranquilo.\n",
            "\n",
            "Mas não consegui entender ainda\n",
            "\n",
            "qual é melhor: se é isto ou aquilo.\n",
            "\n",
            "\n",
            "\n",
            "(Ou Isto ou Aquilo – Cecília Meireles)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNSGytkKjZvy"
      },
      "source": [
        "### Colocando as linhas em uma lista"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mREDvkpcjEED",
        "outputId": "9455819d-0786-4bf0-cac7-d6f544210eee"
      },
      "source": [
        "lista = [ ]\n",
        "infile = open('/content/output.txt','r') \n",
        "for line in infile:\n",
        "  lista.append(line)\n",
        "\n",
        "infile.close()\n",
        "\n",
        "lista  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Ou se tem chuva e não se tem sol,\\n',\n",
              " 'ou se tem sol e não se tem chuva!\\n',\n",
              " 'Ou se calça a luva e não se põe o anel,\\n',\n",
              " 'ou se põe o anel e não se calça a luva!\\n',\n",
              " 'Quem sobe nos ares não fica no chão,\\n',\n",
              " 'quem fica no chão não sobe nos ares.\\n',\n",
              " 'É uma grande pena que não se possa\\n',\n",
              " 'estar ao mesmo tempo nos dois lugares!\\n',\n",
              " 'Ou guardo o dinheiro e não compro o doce,\\n',\n",
              " 'ou compro o doce e gasto o dinheiro.\\n',\n",
              " 'Ou isto ou aquilo: ou isto ou aquilo…\\n',\n",
              " 'e vivo escolhendo o dia inteiro!\\n',\n",
              " 'Não sei se brinco, não sei se estudo,\\n',\n",
              " 'se saio correndo ou fico tranquilo.\\n',\n",
              " 'Mas não consegui entender ainda\\n',\n",
              " 'qual é melhor: se é isto ou aquilo.\\n',\n",
              " '\\n',\n",
              " '(Ou Isto ou Aquilo – Cecília Meireles)']"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSy4UB_6jgVy"
      },
      "source": [
        "### Eliminando *escape characteres*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iHgi9CxjQ36",
        "outputId": "565d6087-444e-45b4-9fa8-819d89b36c82"
      },
      "source": [
        "lista = [ ]\n",
        "infile = open('/content/output.txt','r') \n",
        "for line in infile:\n",
        "  line = line.replace('\\n','')\n",
        "  lista.append(line)\n",
        "\n",
        "infile.close()\n",
        "\n",
        "lista  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Ou se tem chuva e não se tem sol,',\n",
              " 'ou se tem sol e não se tem chuva!',\n",
              " 'Ou se calça a luva e não se põe o anel,',\n",
              " 'ou se põe o anel e não se calça a luva!',\n",
              " 'Quem sobe nos ares não fica no chão,',\n",
              " 'quem fica no chão não sobe nos ares.',\n",
              " 'É uma grande pena que não se possa',\n",
              " 'estar ao mesmo tempo nos dois lugares!',\n",
              " 'Ou guardo o dinheiro e não compro o doce,',\n",
              " 'ou compro o doce e gasto o dinheiro.',\n",
              " 'Ou isto ou aquilo: ou isto ou aquilo…',\n",
              " 'e vivo escolhendo o dia inteiro!',\n",
              " 'Não sei se brinco, não sei se estudo,',\n",
              " 'se saio correndo ou fico tranquilo.',\n",
              " 'Mas não consegui entender ainda',\n",
              " 'qual é melhor: se é isto ou aquilo.',\n",
              " '',\n",
              " '(Ou Isto ou Aquilo – Cecília Meireles)']"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbmL6L5al-Vt"
      },
      "source": [
        "### Split *termos*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBRoZTvOl-Vt",
        "outputId": "31577be2-b726-4192-8cc2-657e533f4976"
      },
      "source": [
        "lista = [ ]\n",
        "infile = open('/content/output.txt','r') \n",
        "for line in infile:\n",
        "  for r in['\\n','.','!']:\n",
        "    line = line.replace(r,'')\n",
        "  for termo in line.split(' '):\n",
        "    lista.append(termo)\n",
        "\n",
        "infile.close()\n",
        "\n",
        "lista  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Ou',\n",
              " 'se',\n",
              " 'tem',\n",
              " 'chuva',\n",
              " 'e',\n",
              " 'não',\n",
              " 'se',\n",
              " 'tem',\n",
              " 'sol,',\n",
              " 'ou',\n",
              " 'se',\n",
              " 'tem',\n",
              " 'sol',\n",
              " 'e',\n",
              " 'não',\n",
              " 'se',\n",
              " 'tem',\n",
              " 'chuva',\n",
              " 'Ou',\n",
              " 'se',\n",
              " 'calça',\n",
              " 'a',\n",
              " 'luva',\n",
              " 'e',\n",
              " 'não',\n",
              " 'se',\n",
              " 'põe',\n",
              " 'o',\n",
              " 'anel,',\n",
              " 'ou',\n",
              " 'se',\n",
              " 'põe',\n",
              " 'o',\n",
              " 'anel',\n",
              " 'e',\n",
              " 'não',\n",
              " 'se',\n",
              " 'calça',\n",
              " 'a',\n",
              " 'luva',\n",
              " 'Quem',\n",
              " 'sobe',\n",
              " 'nos',\n",
              " 'ares',\n",
              " 'não',\n",
              " 'fica',\n",
              " 'no',\n",
              " 'chão,',\n",
              " 'quem',\n",
              " 'fica',\n",
              " 'no',\n",
              " 'chão',\n",
              " 'não',\n",
              " 'sobe',\n",
              " 'nos',\n",
              " 'ares',\n",
              " 'É',\n",
              " 'uma',\n",
              " 'grande',\n",
              " 'pena',\n",
              " 'que',\n",
              " 'não',\n",
              " 'se',\n",
              " 'possa',\n",
              " 'estar',\n",
              " 'ao',\n",
              " 'mesmo',\n",
              " 'tempo',\n",
              " 'nos',\n",
              " 'dois',\n",
              " 'lugares',\n",
              " 'Ou',\n",
              " 'guardo',\n",
              " 'o',\n",
              " 'dinheiro',\n",
              " 'e',\n",
              " 'não',\n",
              " 'compro',\n",
              " 'o',\n",
              " 'doce,',\n",
              " 'ou',\n",
              " 'compro',\n",
              " 'o',\n",
              " 'doce',\n",
              " 'e',\n",
              " 'gasto',\n",
              " 'o',\n",
              " 'dinheiro',\n",
              " 'Ou',\n",
              " 'isto',\n",
              " 'ou',\n",
              " 'aquilo:',\n",
              " 'ou',\n",
              " 'isto',\n",
              " 'ou',\n",
              " 'aquilo…',\n",
              " 'e',\n",
              " 'vivo',\n",
              " 'escolhendo',\n",
              " 'o',\n",
              " 'dia',\n",
              " 'inteiro',\n",
              " 'Não',\n",
              " 'sei',\n",
              " 'se',\n",
              " 'brinco,',\n",
              " 'não',\n",
              " 'sei',\n",
              " 'se',\n",
              " 'estudo,',\n",
              " 'se',\n",
              " 'saio',\n",
              " 'correndo',\n",
              " 'ou',\n",
              " 'fico',\n",
              " 'tranquilo',\n",
              " 'Mas',\n",
              " 'não',\n",
              " 'consegui',\n",
              " 'entender',\n",
              " 'ainda',\n",
              " 'qual',\n",
              " 'é',\n",
              " 'melhor:',\n",
              " 'se',\n",
              " 'é',\n",
              " 'isto',\n",
              " 'ou',\n",
              " 'aquilo',\n",
              " '',\n",
              " '(Ou',\n",
              " 'Isto',\n",
              " 'ou',\n",
              " 'Aquilo',\n",
              " '–',\n",
              " 'Cecília',\n",
              " 'Meireles)']"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nEzlEKdncA_"
      },
      "source": [
        "### Lower e Strip"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-0RPnk5ncA_",
        "outputId": "bdcb1e97-ccb2-4c71-bdbb-90da27b2982c"
      },
      "source": [
        "lista = [ ]\n",
        "infile = open('/content/output.txt','r') \n",
        "for line in infile:\n",
        "  for r in['\\n','.','!']:\n",
        "    line = line.replace(r,'')\n",
        "  line = line.strip()  # Elimina brancos no início ou no final\n",
        "  for termo in line.split(' '):\n",
        "    lista.append(termo.lower())\n",
        "\n",
        "infile.close()\n",
        "\n",
        "lista  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ou',\n",
              " 'se',\n",
              " 'tem',\n",
              " 'chuva',\n",
              " 'e',\n",
              " 'não',\n",
              " 'se',\n",
              " 'tem',\n",
              " 'sol,',\n",
              " 'ou',\n",
              " 'se',\n",
              " 'tem',\n",
              " 'sol',\n",
              " 'e',\n",
              " 'não',\n",
              " 'se',\n",
              " 'tem',\n",
              " 'chuva',\n",
              " 'ou',\n",
              " 'se',\n",
              " 'calça',\n",
              " 'a',\n",
              " 'luva',\n",
              " 'e',\n",
              " 'não',\n",
              " 'se',\n",
              " 'põe',\n",
              " 'o',\n",
              " 'anel,',\n",
              " 'ou',\n",
              " 'se',\n",
              " 'põe',\n",
              " 'o',\n",
              " 'anel',\n",
              " 'e',\n",
              " 'não',\n",
              " 'se',\n",
              " 'calça',\n",
              " 'a',\n",
              " 'luva',\n",
              " 'quem',\n",
              " 'sobe',\n",
              " 'nos',\n",
              " 'ares',\n",
              " 'não',\n",
              " 'fica',\n",
              " 'no',\n",
              " 'chão,',\n",
              " 'quem',\n",
              " 'fica',\n",
              " 'no',\n",
              " 'chão',\n",
              " 'não',\n",
              " 'sobe',\n",
              " 'nos',\n",
              " 'ares',\n",
              " 'é',\n",
              " 'uma',\n",
              " 'grande',\n",
              " 'pena',\n",
              " 'que',\n",
              " 'não',\n",
              " 'se',\n",
              " 'possa',\n",
              " 'estar',\n",
              " 'ao',\n",
              " 'mesmo',\n",
              " 'tempo',\n",
              " 'nos',\n",
              " 'dois',\n",
              " 'lugares',\n",
              " 'ou',\n",
              " 'guardo',\n",
              " 'o',\n",
              " 'dinheiro',\n",
              " 'e',\n",
              " 'não',\n",
              " 'compro',\n",
              " 'o',\n",
              " 'doce,',\n",
              " 'ou',\n",
              " 'compro',\n",
              " 'o',\n",
              " 'doce',\n",
              " 'e',\n",
              " 'gasto',\n",
              " 'o',\n",
              " 'dinheiro',\n",
              " 'ou',\n",
              " 'isto',\n",
              " 'ou',\n",
              " 'aquilo:',\n",
              " 'ou',\n",
              " 'isto',\n",
              " 'ou',\n",
              " 'aquilo…',\n",
              " 'e',\n",
              " 'vivo',\n",
              " 'escolhendo',\n",
              " 'o',\n",
              " 'dia',\n",
              " 'inteiro',\n",
              " 'não',\n",
              " 'sei',\n",
              " 'se',\n",
              " 'brinco,',\n",
              " 'não',\n",
              " 'sei',\n",
              " 'se',\n",
              " 'estudo,',\n",
              " 'se',\n",
              " 'saio',\n",
              " 'correndo',\n",
              " 'ou',\n",
              " 'fico',\n",
              " 'tranquilo',\n",
              " 'mas',\n",
              " 'não',\n",
              " 'consegui',\n",
              " 'entender',\n",
              " 'ainda',\n",
              " 'qual',\n",
              " 'é',\n",
              " 'melhor:',\n",
              " 'se',\n",
              " 'é',\n",
              " 'isto',\n",
              " 'ou',\n",
              " 'aquilo',\n",
              " '',\n",
              " '(ou',\n",
              " 'isto',\n",
              " 'ou',\n",
              " 'aquilo',\n",
              " '–',\n",
              " 'cecília',\n",
              " 'meireles)']"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-juyechp6_0",
        "outputId": "c6d18b9a-758a-4ec9-9553-7781bd33f6c2"
      },
      "source": [
        "lista = [ ]\n",
        "infile = open('/content/output.txt','r') \n",
        "texto = infile.read()\n",
        "\n",
        "for r in['\\n','.','!',',',')','(','-']:\n",
        "  texto = texto.replace(r,' ')\n",
        "for termo in texto.split(' '):\n",
        "  lista.append(termo.lower().strip())\n",
        "\n",
        "infile.close()\n",
        "\n",
        "lista  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ou',\n",
              " 'se',\n",
              " 'tem',\n",
              " 'chuva',\n",
              " 'e',\n",
              " 'não',\n",
              " 'se',\n",
              " 'tem',\n",
              " 'sol',\n",
              " '',\n",
              " 'ou',\n",
              " 'se',\n",
              " 'tem',\n",
              " 'sol',\n",
              " 'e',\n",
              " 'não',\n",
              " 'se',\n",
              " 'tem',\n",
              " 'chuva',\n",
              " '',\n",
              " 'ou',\n",
              " 'se',\n",
              " 'calça',\n",
              " 'a',\n",
              " 'luva',\n",
              " 'e',\n",
              " 'não',\n",
              " 'se',\n",
              " 'põe',\n",
              " 'o',\n",
              " 'anel',\n",
              " '',\n",
              " 'ou',\n",
              " 'se',\n",
              " 'põe',\n",
              " 'o',\n",
              " 'anel',\n",
              " 'e',\n",
              " 'não',\n",
              " 'se',\n",
              " 'calça',\n",
              " 'a',\n",
              " 'luva',\n",
              " '',\n",
              " 'quem',\n",
              " 'sobe',\n",
              " 'nos',\n",
              " 'ares',\n",
              " 'não',\n",
              " 'fica',\n",
              " 'no',\n",
              " 'chão',\n",
              " '',\n",
              " 'quem',\n",
              " 'fica',\n",
              " 'no',\n",
              " 'chão',\n",
              " 'não',\n",
              " 'sobe',\n",
              " 'nos',\n",
              " 'ares',\n",
              " '',\n",
              " 'é',\n",
              " 'uma',\n",
              " 'grande',\n",
              " 'pena',\n",
              " 'que',\n",
              " 'não',\n",
              " 'se',\n",
              " 'possa',\n",
              " 'estar',\n",
              " 'ao',\n",
              " 'mesmo',\n",
              " 'tempo',\n",
              " 'nos',\n",
              " 'dois',\n",
              " 'lugares',\n",
              " '',\n",
              " 'ou',\n",
              " 'guardo',\n",
              " 'o',\n",
              " 'dinheiro',\n",
              " 'e',\n",
              " 'não',\n",
              " 'compro',\n",
              " 'o',\n",
              " 'doce',\n",
              " '',\n",
              " 'ou',\n",
              " 'compro',\n",
              " 'o',\n",
              " 'doce',\n",
              " 'e',\n",
              " 'gasto',\n",
              " 'o',\n",
              " 'dinheiro',\n",
              " '',\n",
              " 'ou',\n",
              " 'isto',\n",
              " 'ou',\n",
              " 'aquilo:',\n",
              " 'ou',\n",
              " 'isto',\n",
              " 'ou',\n",
              " 'aquilo…',\n",
              " 'e',\n",
              " 'vivo',\n",
              " 'escolhendo',\n",
              " 'o',\n",
              " 'dia',\n",
              " 'inteiro',\n",
              " '',\n",
              " 'não',\n",
              " 'sei',\n",
              " 'se',\n",
              " 'brinco',\n",
              " '',\n",
              " 'não',\n",
              " 'sei',\n",
              " 'se',\n",
              " 'estudo',\n",
              " '',\n",
              " 'se',\n",
              " 'saio',\n",
              " 'correndo',\n",
              " 'ou',\n",
              " 'fico',\n",
              " 'tranquilo',\n",
              " '',\n",
              " 'mas',\n",
              " 'não',\n",
              " 'consegui',\n",
              " 'entender',\n",
              " 'ainda',\n",
              " 'qual',\n",
              " 'é',\n",
              " 'melhor:',\n",
              " 'se',\n",
              " 'é',\n",
              " 'isto',\n",
              " 'ou',\n",
              " 'aquilo',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'ou',\n",
              " 'isto',\n",
              " 'ou',\n",
              " 'aquilo',\n",
              " '–',\n",
              " 'cecília',\n",
              " 'meireles',\n",
              " '']"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8e5LmCJrRDl"
      },
      "source": [
        "# `join`, o contrário do `split`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RrwtYbiKrZpU",
        "outputId": "6191799a-fbcd-4823-e1d7-640350faceca"
      },
      "source": [
        "frase = 'Esta é uma frase completa'\n",
        "lista_frase = frase.split(' ')\n",
        "print(lista_frase)\n",
        "\n",
        "frase_reconstruida = '*'.join(lista_frase)\n",
        "print(frase_reconstruida)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Esta', 'é', 'uma', 'frase', 'completa']\n",
            "Esta*é*uma*frase*completa\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0sSsvSnjtiF8"
      },
      "source": [
        "# Remove `` (nulos etc.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kIrqF11TtAEc",
        "outputId": "5e74aec7-edb8-4cde-c34c-76c36ef8896c"
      },
      "source": [
        "lista.remove('')\n",
        "\n",
        "# Cuidado: ERRADO\n",
        "# lista = lista.remove('')\n",
        "\n",
        "print(lista)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ou', 'se', 'tem', 'chuva', 'e', 'não', 'se', 'tem', 'sol', 'ou', 'se', 'tem', 'sol', 'e', 'não', 'se', 'tem', 'chuva', '', 'ou', 'se', 'calça', 'a', 'luva', 'e', 'não', 'se', 'põe', 'o', 'anel', '', 'ou', 'se', 'põe', 'o', 'anel', 'e', 'não', 'se', 'calça', 'a', 'luva', '', 'quem', 'sobe', 'nos', 'ares', 'não', 'fica', 'no', 'chão', '', 'quem', 'fica', 'no', 'chão', 'não', 'sobe', 'nos', 'ares', '', 'é', 'uma', 'grande', 'pena', 'que', 'não', 'se', 'possa', 'estar', 'ao', 'mesmo', 'tempo', 'nos', 'dois', 'lugares', '', 'ou', 'guardo', 'o', 'dinheiro', 'e', 'não', 'compro', 'o', 'doce', '', 'ou', 'compro', 'o', 'doce', 'e', 'gasto', 'o', 'dinheiro', '', 'ou', 'isto', 'ou', 'aquilo:', 'ou', 'isto', 'ou', 'aquilo…', 'e', 'vivo', 'escolhendo', 'o', 'dia', 'inteiro', '', 'não', 'sei', 'se', 'brinco', '', 'não', 'sei', 'se', 'estudo', '', 'se', 'saio', 'correndo', 'ou', 'fico', 'tranquilo', '', 'mas', 'não', 'consegui', 'entender', 'ainda', 'qual', 'é', 'melhor:', 'se', 'é', 'isto', 'ou', 'aquilo', '', '', '', 'ou', 'isto', 'ou', 'aquilo', '–', 'cecília', 'meireles', '']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cw3Joxk5sODU"
      },
      "source": [
        "# Tirando acentos `unicode`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nuowtZUWsYWl",
        "outputId": "773c07da-e7e0-415c-e578-567792ebb4be"
      },
      "source": [
        "!pip install unidecode"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.2-py3-none-any.whl (235 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▍                              | 10 kB 23.1 MB/s eta 0:00:01\r\u001b[K     |██▉                             | 20 kB 26.7 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 30 kB 16.1 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 40 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |███████                         | 51 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 61 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 71 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 81 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 92 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 102 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 112 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 122 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 133 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 143 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 153 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 163 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 174 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 184 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 194 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 204 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 215 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 225 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 235 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 235 kB 6.8 MB/s \n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fyGcWYTxsNPE",
        "outputId": "5bd096c4-6df3-46cb-cd40-d70b56763dfd"
      },
      "source": [
        "from unidecode import unidecode\n",
        "\n",
        "print(unidecode('Café'))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cafe\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1LTArgmvTHl",
        "outputId": "82af0e6c-cff5-4fb5-bf20-1331a87c916b"
      },
      "source": [
        "for i in range(len(lista)):\n",
        "  lista[i] = unidecode(lista[i])\n",
        "\n",
        "lista[0:20]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ou',\n",
              " 'se',\n",
              " 'tem',\n",
              " 'chuva',\n",
              " 'e',\n",
              " 'nao',\n",
              " 'se',\n",
              " 'tem',\n",
              " 'sol',\n",
              " 'ou',\n",
              " 'se',\n",
              " 'tem',\n",
              " 'sol',\n",
              " 'e',\n",
              " 'nao',\n",
              " 'se',\n",
              " 'tem',\n",
              " 'chuva',\n",
              " '',\n",
              " 'ou']"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOwwVeE_uO6V"
      },
      "source": [
        "# Dicionário"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ld8eGAc3ulhN",
        "outputId": "66bf27ee-cea8-4651-d247-e76e84925954"
      },
      "source": [
        "set(lista)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'',\n",
              " 'a',\n",
              " 'ainda',\n",
              " 'anel',\n",
              " 'ao',\n",
              " 'aquilo',\n",
              " 'aquilo:',\n",
              " 'aquilo…',\n",
              " 'ares',\n",
              " 'brinco',\n",
              " 'calça',\n",
              " 'cecília',\n",
              " 'chuva',\n",
              " 'chão',\n",
              " 'compro',\n",
              " 'consegui',\n",
              " 'correndo',\n",
              " 'dia',\n",
              " 'dinheiro',\n",
              " 'doce',\n",
              " 'dois',\n",
              " 'e',\n",
              " 'entender',\n",
              " 'escolhendo',\n",
              " 'estar',\n",
              " 'estudo',\n",
              " 'fica',\n",
              " 'fico',\n",
              " 'gasto',\n",
              " 'grande',\n",
              " 'guardo',\n",
              " 'inteiro',\n",
              " 'isto',\n",
              " 'lugares',\n",
              " 'luva',\n",
              " 'mas',\n",
              " 'meireles',\n",
              " 'melhor:',\n",
              " 'mesmo',\n",
              " 'no',\n",
              " 'nos',\n",
              " 'não',\n",
              " 'o',\n",
              " 'ou',\n",
              " 'pena',\n",
              " 'possa',\n",
              " 'põe',\n",
              " 'qual',\n",
              " 'que',\n",
              " 'quem',\n",
              " 'saio',\n",
              " 'se',\n",
              " 'sei',\n",
              " 'sobe',\n",
              " 'sol',\n",
              " 'tem',\n",
              " 'tempo',\n",
              " 'tranquilo',\n",
              " 'uma',\n",
              " 'vivo',\n",
              " 'é',\n",
              " '–'}"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oy7Fpxtbt_os",
        "outputId": "ddf43a60-4914-49c7-d7ee-c4e21de04976"
      },
      "source": [
        "dic = {}\n",
        "\n",
        "for termo in set(lista):\n",
        "  dic[termo] = lista.count(termo)\n",
        "\n",
        "dic\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'': 16,\n",
              " '-': 1,\n",
              " 'a': 2,\n",
              " 'ainda': 1,\n",
              " 'anel': 2,\n",
              " 'ao': 1,\n",
              " 'aquilo': 2,\n",
              " 'aquilo...': 1,\n",
              " 'aquilo:': 1,\n",
              " 'ares': 2,\n",
              " 'brinco': 1,\n",
              " 'calca': 2,\n",
              " 'cecilia': 1,\n",
              " 'chao': 2,\n",
              " 'chuva': 2,\n",
              " 'compro': 2,\n",
              " 'consegui': 1,\n",
              " 'correndo': 1,\n",
              " 'dia': 1,\n",
              " 'dinheiro': 2,\n",
              " 'doce': 2,\n",
              " 'dois': 1,\n",
              " 'e': 10,\n",
              " 'entender': 1,\n",
              " 'escolhendo': 1,\n",
              " 'estar': 1,\n",
              " 'estudo': 1,\n",
              " 'fica': 2,\n",
              " 'fico': 1,\n",
              " 'gasto': 1,\n",
              " 'grande': 1,\n",
              " 'guardo': 1,\n",
              " 'inteiro': 1,\n",
              " 'isto': 4,\n",
              " 'lugares': 1,\n",
              " 'luva': 2,\n",
              " 'mas': 1,\n",
              " 'meireles': 1,\n",
              " 'melhor:': 1,\n",
              " 'mesmo': 1,\n",
              " 'nao': 11,\n",
              " 'no': 2,\n",
              " 'nos': 3,\n",
              " 'o': 7,\n",
              " 'ou': 14,\n",
              " 'pena': 1,\n",
              " 'poe': 2,\n",
              " 'possa': 1,\n",
              " 'qual': 1,\n",
              " 'que': 1,\n",
              " 'quem': 2,\n",
              " 'saio': 1,\n",
              " 'se': 13,\n",
              " 'sei': 2,\n",
              " 'sobe': 2,\n",
              " 'sol': 2,\n",
              " 'tem': 4,\n",
              " 'tempo': 1,\n",
              " 'tranquilo': 1,\n",
              " 'uma': 1,\n",
              " 'vivo': 1}"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVmKQ9eWhohy"
      },
      "source": [
        "# Aspas Triplas\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "YqGDZldPhyvh",
        "outputId": "7f1ce056-cd04-45d2-8c43-27711286d87f"
      },
      "source": [
        "\"\"\" Este Texto\n",
        "Tem\n",
        "Quebra \"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' Este Texto\\nTem\\nQuebra '"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkpHQWu1wlBX"
      },
      "source": [
        "# NLTK\n",
        "\n",
        "https://www.nltk.org/book/\n",
        "\n",
        "Veja alguns corpus lá:\n",
        "\n",
        "* MacMorpho Corpus\tNILC, USP, Brazil\t1M words, tagged (Brazilian Portuguese)\n",
        "* Movie Reviews\tPang, Lee\t2k movie reviews with sentiment polarity classification\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFsuwD-nwmwu",
        "outputId": "1419cfd3-54d0-4e5b-d6fd-50ab4aef5637"
      },
      "source": [
        "import nltk\n",
        "nltk.download()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> l\n",
            "\n",
            "Packages:\n",
            "  [ ] abc................. Australian Broadcasting Commission 2006\n",
            "  [ ] alpino.............. Alpino Dutch Treebank\n",
            "  [ ] averaged_perceptron_tagger Averaged Perceptron Tagger\n",
            "  [ ] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)\n",
            "  [ ] basque_grammars..... Grammars for Basque\n",
            "  [ ] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n",
            "                           Extraction Systems in Biology)\n",
            "  [ ] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model\n",
            "  [ ] book_grammars....... Grammars from NLTK Book\n",
            "  [ ] brown............... Brown Corpus\n",
            "  [ ] brown_tei........... Brown Corpus (TEI XML Version)\n",
            "  [ ] cess_cat............ CESS-CAT Treebank\n",
            "  [ ] cess_esp............ CESS-ESP Treebank\n",
            "  [ ] chat80.............. Chat-80 Data Files\n",
            "  [ ] city_database....... City Database\n",
            "  [ ] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n",
            "  [ ] comparative_sentences Comparative Sentence Dataset\n",
            "  [ ] comtrans............ ComTrans Corpus Sample\n",
            "  [ ] conll2000........... CONLL 2000 Chunking Corpus\n",
            "  [ ] conll2002........... CONLL 2002 Named Entity Recognition Corpus\n",
            "Hit Enter to continue: \n",
            "  [ ] conll2007........... Dependency Treebanks from CoNLL 2007 (Catalan\n",
            "                           and Basque Subset)\n",
            "  [ ] crubadan............ Crubadan Corpus\n",
            "  [ ] dependency_treebank. Dependency Parsed Treebank\n",
            "  [ ] dolch............... Dolch Word List\n",
            "  [ ] europarl_raw........ Sample European Parliament Proceedings Parallel\n",
            "                           Corpus\n",
            "  [ ] floresta............ Portuguese Treebank\n",
            "  [ ] framenet_v15........ FrameNet 1.5\n",
            "  [ ] framenet_v17........ FrameNet 1.7\n",
            "  [ ] gazetteers.......... Gazeteer Lists\n",
            "  [ ] genesis............. Genesis Corpus\n",
            "  [ ] gutenberg........... Project Gutenberg Selections\n",
            "  [ ] ieer................ NIST IE-ER DATA SAMPLE\n",
            "  [ ] inaugural........... C-Span Inaugural Address Corpus\n",
            "  [ ] indian.............. Indian Language POS-Tagged Corpus\n",
            "  [ ] jeita............... JEITA Public Morphologically Tagged Corpus (in\n",
            "                           ChaSen format)\n",
            "  [ ] kimmo............... PC-KIMMO Data Files\n",
            "  [ ] knbc................ KNB Corpus (Annotated blog corpus)\n",
            "  [ ] large_grammars...... Large context-free and feature-based grammars\n",
            "                           for parser comparison\n",
            "Hit Enter to continue: \n",
            "  [ ] lin_thesaurus....... Lin's Dependency Thesaurus\n",
            "  [ ] mac_morpho.......... MAC-MORPHO: Brazilian Portuguese news text with\n",
            "                           part-of-speech tags\n",
            "  [ ] machado............. Machado de Assis -- Obra Completa\n",
            "  [ ] masc_tagged......... MASC Tagged Corpus\n",
            "  [ ] maxent_ne_chunker... ACE Named Entity Chunker (Maximum entropy)\n",
            "  [ ] maxent_treebank_pos_tagger Treebank Part of Speech Tagger (Maximum entropy)\n",
            "  [ ] moses_sample........ Moses Sample Models\n",
            "  [ ] movie_reviews....... Sentiment Polarity Dataset Version 2.0\n",
            "  [ ] mte_teip5........... MULTEXT-East 1984 annotated corpus 4.0\n",
            "  [ ] mwa_ppdb............ The monolingual word aligner (Sultan et al.\n",
            "                           2015) subset of the Paraphrase Database.\n",
            "  [ ] names............... Names Corpus, Version 1.3 (1994-03-29)\n",
            "  [ ] nombank.1.0......... NomBank Corpus 1.0\n",
            "  [ ] nonbreaking_prefixes Non-Breaking Prefixes (Moses Decoder)\n",
            "  [ ] nps_chat............ NPS Chat\n",
            "  [ ] omw................. Open Multilingual Wordnet\n",
            "  [ ] opinion_lexicon..... Opinion Lexicon\n",
            "  [ ] panlex_swadesh...... PanLex Swadesh Corpora\n",
            "  [ ] paradigms........... Paradigm Corpus\n",
            "  [ ] pe08................ Cross-Framework and Cross-Domain Parser\n",
            "                           Evaluation Shared Task\n",
            "Hit Enter to continue: \n",
            "  [ ] perluniprops........ perluniprops: Index of Unicode Version 7.0.0\n",
            "                           character properties in Perl\n",
            "  [ ] pil................. The Patient Information Leaflet (PIL) Corpus\n",
            "  [ ] pl196x.............. Polish language of the XX century sixties\n",
            "  [ ] porter_test......... Porter Stemmer Test Files\n",
            "  [ ] ppattach............ Prepositional Phrase Attachment Corpus\n",
            "  [ ] problem_reports..... Problem Report Corpus\n",
            "  [ ] product_reviews_1... Product Reviews (5 Products)\n",
            "  [ ] product_reviews_2... Product Reviews (9 Products)\n",
            "  [ ] propbank............ Proposition Bank Corpus 1.0\n",
            "  [ ] pros_cons........... Pros and Cons\n",
            "  [ ] ptb................. Penn Treebank\n",
            "  [ ] punkt............... Punkt Tokenizer Models\n",
            "  [ ] qc.................. Experimental Data for Question Classification\n",
            "  [ ] reuters............. The Reuters-21578 benchmark corpus, ApteMod\n",
            "                           version\n",
            "  [ ] rslp................ RSLP Stemmer (Removedor de Sufixos da Lingua\n",
            "                           Portuguesa)\n",
            "  [ ] rte................. PASCAL RTE Challenges 1, 2, and 3\n",
            "  [ ] sample_grammars..... Sample Grammars\n",
            "  [ ] semcor.............. SemCor 3.0\n",
            "Hit Enter to continue: \n",
            "  [ ] senseval............ SENSEVAL 2 Corpus: Sense Tagged Text\n",
            "  [ ] sentence_polarity... Sentence Polarity Dataset v1.0\n",
            "  [ ] sentiwordnet........ SentiWordNet\n",
            "  [ ] shakespeare......... Shakespeare XML Corpus Sample\n",
            "  [ ] sinica_treebank..... Sinica Treebank Corpus Sample\n",
            "  [ ] smultron............ SMULTRON Corpus Sample\n",
            "  [ ] snowball_data....... Snowball Data\n",
            "  [ ] spanish_grammars.... Grammars for Spanish\n",
            "  [ ] state_union......... C-Span State of the Union Address Corpus\n",
            "  [ ] stopwords........... Stopwords Corpus\n",
            "  [ ] subjectivity........ Subjectivity Dataset v1.0\n",
            "  [ ] swadesh............. Swadesh Wordlists\n",
            "  [ ] switchboard......... Switchboard Corpus Sample\n",
            "  [ ] tagsets............. Help on Tagsets\n",
            "  [ ] timit............... TIMIT Corpus Sample\n",
            "  [ ] toolbox............. Toolbox Sample Files\n",
            "  [ ] treebank............ Penn Treebank Sample\n",
            "  [ ] twitter_samples..... Twitter Samples\n",
            "  [ ] udhr2............... Universal Declaration of Human Rights Corpus\n",
            "                           (Unicode Version)\n",
            "  [ ] udhr................ Universal Declaration of Human Rights Corpus\n",
            "Hit Enter to continue: \n",
            "  [ ] unicode_samples..... Unicode Samples\n",
            "  [ ] universal_tagset.... Mappings to the Universal Part-of-Speech Tagset\n",
            "  [ ] universal_treebanks_v20 Universal Treebanks Version 2.0\n",
            "  [ ] vader_lexicon....... VADER Sentiment Lexicon\n",
            "  [ ] verbnet3............ VerbNet Lexicon, Version 3.3\n",
            "  [ ] verbnet............. VerbNet Lexicon, Version 2.1\n",
            "  [ ] webtext............. Web Text Corpus\n",
            "  [ ] wmt15_eval.......... Evaluation data from WMT15\n",
            "  [ ] word2vec_sample..... Word2Vec Sample\n",
            "  [ ] wordnet............. WordNet\n",
            "  [ ] wordnet_ic.......... WordNet-InfoContent\n",
            "  [ ] words............... Word Lists\n",
            "  [ ] ycoe................ York-Toronto-Helsinki Parsed Corpus of Old\n",
            "                           English Prose\n",
            "\n",
            "Collections:\n",
            "  [ ] all-corpora......... All the corpora\n",
            "  [ ] all-nltk............ All packages available on nltk_data gh-pages\n",
            "                           branch\n",
            "  [ ] all................. All packages\n",
            "  [ ] book................ Everything used in the NLTK Book\n",
            "  [ ] popular............. Popular packages\n",
            "Hit Enter to continue: \n",
            "Downloader> \n",
            "  [ ] tests............... Packages for running tests\n",
            "  [ ] third-party......... Third-party data packages\n",
            "\n",
            "([*] marks installed packages)\n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o43unYoQ0GT_"
      },
      "source": [
        "# Download 'all'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gA_ocSiFyo3u",
        "outputId": "ef938a70-8cba-4cf0-ef84-68d35fab9672"
      },
      "source": [
        "nltk.download('all')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAz29866z9S-"
      },
      "source": [
        "# Acessando um Corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjUQQLCZ2iBf",
        "outputId": "39912478-1597-4a91-cd4e-de96c934bb78"
      },
      "source": [
        "print(nltk.corpus.mac_morpho.readme())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAC-MORPHO: Brazilian Portuguese news text with part-of-speech tags\n",
            "\n",
            "1,167,183 words of journalistic texts extracted from ten sections of\n",
            "the daily newspaper Folha de Sau Paulo, 1994.  (Version for training taggers.)\n",
            "\n",
            "http://www.nilc.icmc.usp.br/lacioweb/\n",
            "\n",
            "Distributed with permission of\n",
            "NÃºcleo Interinstitucional de LingÃ¼Ã­stica Computacional (NILC),\n",
            "Universidade de SÃ£o Paulo (USP) in SÃ£o Carlos,\n",
            "Universidade Federal de SÃ£o Carlos (UFSCar),\n",
            "Universidade Estadual Paulista (UNESP) of Araraquara.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2Y89C5Pz5E-",
        "outputId": "a1624083-a784-497e-eb2d-0e55eb71c4c6"
      },
      "source": [
        "nltk.corpus.mac_morpho"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<MacMorphoCorpusReader in '/root/nltk_data/corpora/mac_morpho'>"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rohM5o640aPO",
        "outputId": "79aba997-88f1-431a-f5ca-48bfaa209c59"
      },
      "source": [
        "dir(nltk.corpus.mac_morpho)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['__class__',\n",
              " '__delattr__',\n",
              " '__dict__',\n",
              " '__dir__',\n",
              " '__doc__',\n",
              " '__eq__',\n",
              " '__format__',\n",
              " '__ge__',\n",
              " '__getattribute__',\n",
              " '__gt__',\n",
              " '__hash__',\n",
              " '__init__',\n",
              " '__init_subclass__',\n",
              " '__le__',\n",
              " '__lt__',\n",
              " '__module__',\n",
              " '__ne__',\n",
              " '__new__',\n",
              " '__reduce__',\n",
              " '__reduce_ex__',\n",
              " '__repr__',\n",
              " '__setattr__',\n",
              " '__sizeof__',\n",
              " '__str__',\n",
              " '__subclasshook__',\n",
              " '__unicode__',\n",
              " '__weakref__',\n",
              " '_encoding',\n",
              " '_fileids',\n",
              " '_get_root',\n",
              " '_para_block_reader',\n",
              " '_read_block',\n",
              " '_root',\n",
              " '_sent_tokenizer',\n",
              " '_sep',\n",
              " '_tagset',\n",
              " '_unload',\n",
              " '_word_tokenizer',\n",
              " 'abspath',\n",
              " 'abspaths',\n",
              " 'citation',\n",
              " 'encoding',\n",
              " 'ensure_loaded',\n",
              " 'fileids',\n",
              " 'license',\n",
              " 'open',\n",
              " 'paras',\n",
              " 'raw',\n",
              " 'readme',\n",
              " 'root',\n",
              " 'sents',\n",
              " 'tagged_paras',\n",
              " 'tagged_sents',\n",
              " 'tagged_words',\n",
              " 'unicode_repr',\n",
              " 'words']"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8J-jE1t0gCl",
        "outputId": "b0cdf729-3e8e-4463-cc7a-1e98c75d3fa7"
      },
      "source": [
        "nltk.corpus.mac_morpho.words() # lista palavras do corpus"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Jersei', 'atinge', 'média', 'de', 'Cr$', '1,4', ...]"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iataqdl-0vhH",
        "outputId": "66a249ec-2885-4600-fba7-307d23830201"
      },
      "source": [
        "len( nltk.corpus.mac_morpho.words() ) # quantas palavras"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1170095"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1n3xX2di00X2",
        "outputId": "c3051d3c-a4dd-4a40-a286-3329564eb9aa"
      },
      "source": [
        "nltk.corpus.mac_morpho.sents() # sentenças, lista de lista de palavras"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Jersei', 'atinge', 'média', 'de', 'Cr$', '1,4', 'milhão', 'em', 'a', 'venda', 'de', 'a', 'Pinhal', 'em', 'São', 'Paulo'], ['Programe', 'sua', 'viagem', 'a', 'a', 'Exposição', 'Nacional', 'do', 'Zebu', ',', 'que', 'começa', 'dia', '25'], ...]"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJIJnFzY1Him",
        "outputId": "164a65e1-82f0-4954-bb67-92f294197201"
      },
      "source": [
        "len( nltk.corpus.mac_morpho.sents() ) # nr de sentenças "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "51397"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKgkzHm51Q2b",
        "outputId": "aa90668d-99b3-49dc-86cb-72d7b8488264"
      },
      "source": [
        "print( ' '.join(nltk.corpus.mac_morpho.sents()[0]) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jersei atinge média de Cr$ 1,4 milhão em a venda de a Pinhal em São Paulo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHkqdYdi1iRK",
        "outputId": "5e3bfa38-e139-4a24-ab9d-70ac2c115f3a"
      },
      "source": [
        "for i in range(10):\n",
        "  print( ' '.join(nltk.corpus.mac_morpho.sents()[i]) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jersei atinge média de Cr$ 1,4 milhão em a venda de a Pinhal em São Paulo\n",
            "Programe sua viagem a a Exposição Nacional do Zebu , que começa dia 25\n",
            "Safra recorde e disponibilidade de crédito ativam vendas de máquinas agrícolas\n",
            "A degradação de as terras por o mau uso de os solos avança em o\n",
            "A desertificação tornou crítica a produtividade de 52 mil km² em a região\n",
            "Em o dia 15 , Dia da Conservação do Solo , o único fato a festejar pode ser a Convenção Internacional sobre Desertificação\n",
            "A produção brasileira de pintos de corte totalizou , em fevereiro último , 166 milhões , volume 6,79 % superior a o registrado em fevereiro de 93 , segundo dados de a Associação Brasileira dos Produtores de Pinto de Corte ( Apinco )\n",
            "A Apinco destaca em seu boletim mensal que o setor avícola está otimista com o atual programa de estabilização econômica\n",
            "\" A melhoria de o padrão aquisitivo resulta em maior demanda de carne de frango \"\n",
            "O secretário de a agricultura paulista , Roberto Rodrigues , aprovou o pacote de o trigo , anunciado em o final de março por o governo federal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IiugFcOz1n-m",
        "outputId": "161203d8-fa2b-4280-85a4-b96de4df7f95"
      },
      "source": [
        "nltk.corpus.mac_morpho.tagged_words() # 'N' nome, 'V' verbo, 'ART' artigo, 'PREP' preposição, 'NPROP' nome próprio ..."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Jersei', 'N'), ('atinge', 'V'), ('média', 'N'), ...]"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PVAUZWne11fP",
        "outputId": "e1eb9a8d-b95c-4cef-ae6e-312ad5cd3d28"
      },
      "source": [
        "nltk.corpus.mac_morpho.tagged_sents() # 'N' nome, 'V' verbo, ..."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[('Jersei', 'N'), ('atinge', 'V'), ('média', 'N'), ('de', 'PREP'), ('Cr$', 'CUR'), ('1,4', 'NUM'), ('milhão', 'N'), ('em', 'PREP|+'), ('a', 'ART'), ('venda', 'N'), ('de', 'PREP|+'), ('a', 'ART'), ('Pinhal', 'NPROP'), ('em', 'PREP'), ('São', 'NPROP'), ('Paulo', 'NPROP')], [('Programe', 'V'), ('sua', 'PROADJ'), ('viagem', 'N'), ('a', 'PREP|+'), ('a', 'ART'), ('Exposição', 'NPROP'), ('Nacional', 'NPROP'), ('do', 'NPROP'), ('Zebu', 'NPROP'), (',', ','), ('que', 'PRO-KS-REL'), ('começa', 'V'), ('dia', 'N'), ('25', 'N|AP')], ...]"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsc82e2W-r1V"
      },
      "source": [
        "# Token(inzação) $\\times$ `split`\n",
        "\n",
        "Tokens, são não somente palavras, mas também pontuações, números, valores etc. e é melhor que o `split()`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXqSWW7U_B4w",
        "outputId": "de95fa10-5d80-48f9-9761-9f34c1b0fce3"
      },
      "source": [
        "nltk.word_tokenize('Esta é a nossa frase para Tokenizar, ela tem símbolos ; * e ainda tem valores, R$ 10.3 USD')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Esta',\n",
              " 'é',\n",
              " 'a',\n",
              " 'nossa',\n",
              " 'frase',\n",
              " 'para',\n",
              " 'Tokenizar',\n",
              " ',',\n",
              " 'ela',\n",
              " 'tem',\n",
              " 'símbolos',\n",
              " ';',\n",
              " '*',\n",
              " 'e',\n",
              " 'ainda',\n",
              " 'tem',\n",
              " 'valores',\n",
              " ',',\n",
              " 'R',\n",
              " '$',\n",
              " '10.3',\n",
              " 'USD']"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gl_xeN4H_Yko",
        "outputId": "5cc77308-1c17-48b8-e482-e47b58d74027"
      },
      "source": [
        "nltk.word_tokenize(' '.join(nltk.corpus.mac_morpho.sents()[0]) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Jersei',\n",
              " 'atinge',\n",
              " 'média',\n",
              " 'de',\n",
              " 'Cr',\n",
              " '$',\n",
              " '1,4',\n",
              " 'milhão',\n",
              " 'em',\n",
              " 'a',\n",
              " 'venda',\n",
              " 'de',\n",
              " 'a',\n",
              " 'Pinhal',\n",
              " 'em',\n",
              " 'São',\n",
              " 'Paulo']"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPXaf_toAEvB",
        "outputId": "f14d4f62-ff35-4024-c367-a8541361fb34"
      },
      "source": [
        "infile = open('/content/output.txt','r')\n",
        "texto = infile.read()\n",
        "infile.close()\n",
        "\n",
        "nltk.word_tokenize(texto)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Ou',\n",
              " 'se',\n",
              " 'tem',\n",
              " 'chuva',\n",
              " 'e',\n",
              " 'não',\n",
              " 'se',\n",
              " 'tem',\n",
              " 'sol',\n",
              " ',',\n",
              " 'ou',\n",
              " 'se',\n",
              " 'tem',\n",
              " 'sol',\n",
              " 'e',\n",
              " 'não',\n",
              " 'se',\n",
              " 'tem',\n",
              " 'chuva',\n",
              " '!',\n",
              " 'Ou',\n",
              " 'se',\n",
              " 'calça',\n",
              " 'a',\n",
              " 'luva',\n",
              " 'e',\n",
              " 'não',\n",
              " 'se',\n",
              " 'põe',\n",
              " 'o',\n",
              " 'anel',\n",
              " ',',\n",
              " 'ou',\n",
              " 'se',\n",
              " 'põe',\n",
              " 'o',\n",
              " 'anel',\n",
              " 'e',\n",
              " 'não',\n",
              " 'se',\n",
              " 'calça',\n",
              " 'a',\n",
              " 'luva',\n",
              " '!',\n",
              " 'Quem',\n",
              " 'sobe',\n",
              " 'nos',\n",
              " 'ares',\n",
              " 'não',\n",
              " 'fica',\n",
              " 'no',\n",
              " 'chão',\n",
              " ',',\n",
              " 'quem',\n",
              " 'fica',\n",
              " 'no',\n",
              " 'chão',\n",
              " 'não',\n",
              " 'sobe',\n",
              " 'nos',\n",
              " 'ares',\n",
              " '.',\n",
              " 'É',\n",
              " 'uma',\n",
              " 'grande',\n",
              " 'pena',\n",
              " 'que',\n",
              " 'não',\n",
              " 'se',\n",
              " 'possa',\n",
              " 'estar',\n",
              " 'ao',\n",
              " 'mesmo',\n",
              " 'tempo',\n",
              " 'nos',\n",
              " 'dois',\n",
              " 'lugares',\n",
              " '!',\n",
              " 'Ou',\n",
              " 'guardo',\n",
              " 'o',\n",
              " 'dinheiro',\n",
              " 'e',\n",
              " 'não',\n",
              " 'compro',\n",
              " 'o',\n",
              " 'doce',\n",
              " ',',\n",
              " 'ou',\n",
              " 'compro',\n",
              " 'o',\n",
              " 'doce',\n",
              " 'e',\n",
              " 'gasto',\n",
              " 'o',\n",
              " 'dinheiro',\n",
              " '.',\n",
              " 'Ou',\n",
              " 'isto',\n",
              " 'ou',\n",
              " 'aquilo',\n",
              " ':',\n",
              " 'ou',\n",
              " 'isto',\n",
              " 'ou',\n",
              " 'aquilo…',\n",
              " 'e',\n",
              " 'vivo',\n",
              " 'escolhendo',\n",
              " 'o',\n",
              " 'dia',\n",
              " 'inteiro',\n",
              " '!',\n",
              " 'Não',\n",
              " 'sei',\n",
              " 'se',\n",
              " 'brinco',\n",
              " ',',\n",
              " 'não',\n",
              " 'sei',\n",
              " 'se',\n",
              " 'estudo',\n",
              " ',',\n",
              " 'se',\n",
              " 'saio',\n",
              " 'correndo',\n",
              " 'ou',\n",
              " 'fico',\n",
              " 'tranquilo',\n",
              " '.',\n",
              " 'Mas',\n",
              " 'não',\n",
              " 'consegui',\n",
              " 'entender',\n",
              " 'ainda',\n",
              " 'qual',\n",
              " 'é',\n",
              " 'melhor',\n",
              " ':',\n",
              " 'se',\n",
              " 'é',\n",
              " 'isto',\n",
              " 'ou',\n",
              " 'aquilo',\n",
              " '.',\n",
              " '(',\n",
              " 'Ou',\n",
              " 'Isto',\n",
              " 'ou',\n",
              " 'Aquilo',\n",
              " '–',\n",
              " 'Cecília',\n",
              " 'Meireles',\n",
              " ')']"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfxJ5VnnAUZA"
      },
      "source": [
        "# Empregando Expressões Regulares\n",
        "\n",
        "Acesse https://pythex.org/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCRqWcNZAVC5"
      },
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "tokenizer = RegexpTokenizer(r'\\w+') # elimina pontuações! \n",
        "\n",
        "tokens = tokenizer.tokenize(texto)\n",
        "tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfyX3G6fDcXk"
      },
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "tokenizer = RegexpTokenizer(r'[A-z]\\w*') # elimina pontuações e números\n",
        "# tokenizer = RegexpTokenizer(r'[a-zA-Z]\\w*') # mesmo resultado!\n",
        "\n",
        "print(' '.join(nltk.corpus.mac_morpho.sents()[0]))\n",
        "tokens = tokenizer.tokenize(' '.join(nltk.corpus.mac_morpho.sents()[0]) )\n",
        "tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gq-97ilwElSa",
        "outputId": "fdbe6787-25e6-4a39-a90a-911986a2735f"
      },
      "source": [
        "tokens = tokenizer.tokenize('www.mackenzie.br')\n",
        "tokens"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['www', 'mackenzie', 'br']"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sP6NKh2BFLbJ"
      },
      "source": [
        "# Frequencia de Termos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gH-fBSsaFKJq",
        "outputId": "ee450e19-5629-44c6-e16c-849bad236ead"
      },
      "source": [
        "nltk.FreqDist(tokenizer.tokenize(texto))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FreqDist({'Aquilo': 1,\n",
              "          'Cecília': 1,\n",
              "          'Isto': 1,\n",
              "          'Mas': 1,\n",
              "          'Meireles': 1,\n",
              "          'Não': 1,\n",
              "          'Ou': 5,\n",
              "          'Quem': 1,\n",
              "          'a': 2,\n",
              "          'ainda': 1,\n",
              "          'anel': 2,\n",
              "          'ao': 1,\n",
              "          'aquilo': 3,\n",
              "          'ares': 2,\n",
              "          'brinco': 1,\n",
              "          'calça': 2,\n",
              "          'chuva': 2,\n",
              "          'chão': 2,\n",
              "          'compro': 2,\n",
              "          'consegui': 1,\n",
              "          'correndo': 1,\n",
              "          'dia': 1,\n",
              "          'dinheiro': 2,\n",
              "          'doce': 2,\n",
              "          'dois': 1,\n",
              "          'e': 7,\n",
              "          'entender': 1,\n",
              "          'escolhendo': 1,\n",
              "          'estar': 1,\n",
              "          'estudo': 1,\n",
              "          'fica': 2,\n",
              "          'fico': 1,\n",
              "          'gasto': 1,\n",
              "          'grande': 1,\n",
              "          'guardo': 1,\n",
              "          'inteiro': 1,\n",
              "          'isto': 3,\n",
              "          'lugares': 1,\n",
              "          'luva': 2,\n",
              "          'melhor': 1,\n",
              "          'mesmo': 1,\n",
              "          'no': 2,\n",
              "          'nos': 3,\n",
              "          'não': 10,\n",
              "          'o': 7,\n",
              "          'ou': 9,\n",
              "          'pena': 1,\n",
              "          'possa': 1,\n",
              "          'põe': 2,\n",
              "          'qual': 1,\n",
              "          'que': 1,\n",
              "          'quem': 1,\n",
              "          'saio': 1,\n",
              "          'se': 13,\n",
              "          'sei': 2,\n",
              "          'sobe': 2,\n",
              "          'sol': 2,\n",
              "          'tem': 4,\n",
              "          'tempo': 1,\n",
              "          'tranquilo': 1,\n",
              "          'uma': 1,\n",
              "          'vivo': 1})"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0opFzjp4GHk4",
        "outputId": "1682e6e8-60de-4585-dfc0-3a7665aecf91"
      },
      "source": [
        "freq = nltk.FreqDist(tokenizer.tokenize(texto))\n",
        "\n",
        "# freq.most_common() # all\n",
        "freq.most_common(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('se', 13),\n",
              " ('não', 10),\n",
              " ('ou', 9),\n",
              " ('e', 7),\n",
              " ('o', 7),\n",
              " ('Ou', 5),\n",
              " ('tem', 4),\n",
              " ('nos', 3),\n",
              " ('isto', 3),\n",
              " ('aquilo', 3)]"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0E7UgOnvGpcR",
        "outputId": "1c50d171-89ed-489f-ee49-052cd9bcd699"
      },
      "source": [
        "tokens = tokenizer.tokenize(texto)\n",
        "freq = nltk.FreqDist(w.lower() for w in tokens)\n",
        "\n",
        "# freq.most_common() # all\n",
        "freq.most_common(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('ou', 14),\n",
              " ('se', 13),\n",
              " ('não', 11),\n",
              " ('e', 7),\n",
              " ('o', 7),\n",
              " ('tem', 4),\n",
              " ('isto', 4),\n",
              " ('aquilo', 4),\n",
              " ('nos', 3),\n",
              " ('chuva', 2)]"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2E_41fCkHOz6"
      },
      "source": [
        "# Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qI7OAM8dHQyW",
        "outputId": "7fd3b261-6c4d-428d-92df-2dd1e1849339"
      },
      "source": [
        "nltk.corpus.stopwords.words('portuguese') "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['de',\n",
              " 'a',\n",
              " 'o',\n",
              " 'que',\n",
              " 'e',\n",
              " 'é',\n",
              " 'do',\n",
              " 'da',\n",
              " 'em',\n",
              " 'um',\n",
              " 'para',\n",
              " 'com',\n",
              " 'não',\n",
              " 'uma',\n",
              " 'os',\n",
              " 'no',\n",
              " 'se',\n",
              " 'na',\n",
              " 'por',\n",
              " 'mais',\n",
              " 'as',\n",
              " 'dos',\n",
              " 'como',\n",
              " 'mas',\n",
              " 'ao',\n",
              " 'ele',\n",
              " 'das',\n",
              " 'à',\n",
              " 'seu',\n",
              " 'sua',\n",
              " 'ou',\n",
              " 'quando',\n",
              " 'muito',\n",
              " 'nos',\n",
              " 'já',\n",
              " 'eu',\n",
              " 'também',\n",
              " 'só',\n",
              " 'pelo',\n",
              " 'pela',\n",
              " 'até',\n",
              " 'isso',\n",
              " 'ela',\n",
              " 'entre',\n",
              " 'depois',\n",
              " 'sem',\n",
              " 'mesmo',\n",
              " 'aos',\n",
              " 'seus',\n",
              " 'quem',\n",
              " 'nas',\n",
              " 'me',\n",
              " 'esse',\n",
              " 'eles',\n",
              " 'você',\n",
              " 'essa',\n",
              " 'num',\n",
              " 'nem',\n",
              " 'suas',\n",
              " 'meu',\n",
              " 'às',\n",
              " 'minha',\n",
              " 'numa',\n",
              " 'pelos',\n",
              " 'elas',\n",
              " 'qual',\n",
              " 'nós',\n",
              " 'lhe',\n",
              " 'deles',\n",
              " 'essas',\n",
              " 'esses',\n",
              " 'pelas',\n",
              " 'este',\n",
              " 'dele',\n",
              " 'tu',\n",
              " 'te',\n",
              " 'vocês',\n",
              " 'vos',\n",
              " 'lhes',\n",
              " 'meus',\n",
              " 'minhas',\n",
              " 'teu',\n",
              " 'tua',\n",
              " 'teus',\n",
              " 'tuas',\n",
              " 'nosso',\n",
              " 'nossa',\n",
              " 'nossos',\n",
              " 'nossas',\n",
              " 'dela',\n",
              " 'delas',\n",
              " 'esta',\n",
              " 'estes',\n",
              " 'estas',\n",
              " 'aquele',\n",
              " 'aquela',\n",
              " 'aqueles',\n",
              " 'aquelas',\n",
              " 'isto',\n",
              " 'aquilo',\n",
              " 'estou',\n",
              " 'está',\n",
              " 'estamos',\n",
              " 'estão',\n",
              " 'estive',\n",
              " 'esteve',\n",
              " 'estivemos',\n",
              " 'estiveram',\n",
              " 'estava',\n",
              " 'estávamos',\n",
              " 'estavam',\n",
              " 'estivera',\n",
              " 'estivéramos',\n",
              " 'esteja',\n",
              " 'estejamos',\n",
              " 'estejam',\n",
              " 'estivesse',\n",
              " 'estivéssemos',\n",
              " 'estivessem',\n",
              " 'estiver',\n",
              " 'estivermos',\n",
              " 'estiverem',\n",
              " 'hei',\n",
              " 'há',\n",
              " 'havemos',\n",
              " 'hão',\n",
              " 'houve',\n",
              " 'houvemos',\n",
              " 'houveram',\n",
              " 'houvera',\n",
              " 'houvéramos',\n",
              " 'haja',\n",
              " 'hajamos',\n",
              " 'hajam',\n",
              " 'houvesse',\n",
              " 'houvéssemos',\n",
              " 'houvessem',\n",
              " 'houver',\n",
              " 'houvermos',\n",
              " 'houverem',\n",
              " 'houverei',\n",
              " 'houverá',\n",
              " 'houveremos',\n",
              " 'houverão',\n",
              " 'houveria',\n",
              " 'houveríamos',\n",
              " 'houveriam',\n",
              " 'sou',\n",
              " 'somos',\n",
              " 'são',\n",
              " 'era',\n",
              " 'éramos',\n",
              " 'eram',\n",
              " 'fui',\n",
              " 'foi',\n",
              " 'fomos',\n",
              " 'foram',\n",
              " 'fora',\n",
              " 'fôramos',\n",
              " 'seja',\n",
              " 'sejamos',\n",
              " 'sejam',\n",
              " 'fosse',\n",
              " 'fôssemos',\n",
              " 'fossem',\n",
              " 'for',\n",
              " 'formos',\n",
              " 'forem',\n",
              " 'serei',\n",
              " 'será',\n",
              " 'seremos',\n",
              " 'serão',\n",
              " 'seria',\n",
              " 'seríamos',\n",
              " 'seriam',\n",
              " 'tenho',\n",
              " 'tem',\n",
              " 'temos',\n",
              " 'tém',\n",
              " 'tinha',\n",
              " 'tínhamos',\n",
              " 'tinham',\n",
              " 'tive',\n",
              " 'teve',\n",
              " 'tivemos',\n",
              " 'tiveram',\n",
              " 'tivera',\n",
              " 'tivéramos',\n",
              " 'tenha',\n",
              " 'tenhamos',\n",
              " 'tenham',\n",
              " 'tivesse',\n",
              " 'tivéssemos',\n",
              " 'tivessem',\n",
              " 'tiver',\n",
              " 'tivermos',\n",
              " 'tiverem',\n",
              " 'terei',\n",
              " 'terá',\n",
              " 'teremos',\n",
              " 'terão',\n",
              " 'teria',\n",
              " 'teríamos',\n",
              " 'teriam']"
            ]
          },
          "metadata": {},
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ug4bsXTIHzVD"
      },
      "source": [
        "# Termos sem stopwords "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HO-U_F4pHyuK",
        "outputId": "d4331171-c7e9-4050-b7b3-d10465dc1ba0"
      },
      "source": [
        "tokens = tokenizer.tokenize(texto)\n",
        "stopwords = nltk.corpus.stopwords.words('portuguese') \n",
        "\n",
        "tokens_sem_stopwords = [w.lower() for w in tokens if w not in stopwords]\n",
        "freq = nltk.FreqDist(tokens_sem_stopwords)\n",
        "\n",
        "# freq.most_common() # all\n",
        "freq.most_common(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('ou', 5),\n",
              " ('chuva', 2),\n",
              " ('sol', 2),\n",
              " ('calça', 2),\n",
              " ('luva', 2),\n",
              " ('põe', 2),\n",
              " ('anel', 2),\n",
              " ('sobe', 2),\n",
              " ('ares', 2),\n",
              " ('fica', 2)]"
            ]
          },
          "metadata": {},
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uwyLrlqIkPb"
      },
      "source": [
        "# Bi, Tri e n-gramas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyS0b8JgIovL"
      },
      "source": [
        "from nltk import bigrams\n",
        "from nltk import trigrams\n",
        "from nltk import ngrams"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AIF2c0yQIzNj",
        "outputId": "bc368ae1-babd-48d9-8ea2-ab537bdc21c7"
      },
      "source": [
        "list( bigrams(tokens_sem_stopwords) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('ou', 'chuva'),\n",
              " ('chuva', 'sol'),\n",
              " ('sol', 'sol'),\n",
              " ('sol', 'chuva'),\n",
              " ('chuva', 'ou'),\n",
              " ('ou', 'calça'),\n",
              " ('calça', 'luva'),\n",
              " ('luva', 'põe'),\n",
              " ('põe', 'anel'),\n",
              " ('anel', 'põe'),\n",
              " ('põe', 'anel'),\n",
              " ('anel', 'calça'),\n",
              " ('calça', 'luva'),\n",
              " ('luva', 'quem'),\n",
              " ('quem', 'sobe'),\n",
              " ('sobe', 'ares'),\n",
              " ('ares', 'fica'),\n",
              " ('fica', 'chão'),\n",
              " ('chão', 'fica'),\n",
              " ('fica', 'chão'),\n",
              " ('chão', 'sobe'),\n",
              " ('sobe', 'ares'),\n",
              " ('ares', 'grande'),\n",
              " ('grande', 'pena'),\n",
              " ('pena', 'possa'),\n",
              " ('possa', 'estar'),\n",
              " ('estar', 'tempo'),\n",
              " ('tempo', 'dois'),\n",
              " ('dois', 'lugares'),\n",
              " ('lugares', 'ou'),\n",
              " ('ou', 'guardo'),\n",
              " ('guardo', 'dinheiro'),\n",
              " ('dinheiro', 'compro'),\n",
              " ('compro', 'doce'),\n",
              " ('doce', 'compro'),\n",
              " ('compro', 'doce'),\n",
              " ('doce', 'gasto'),\n",
              " ('gasto', 'dinheiro'),\n",
              " ('dinheiro', 'ou'),\n",
              " ('ou', 'vivo'),\n",
              " ('vivo', 'escolhendo'),\n",
              " ('escolhendo', 'dia'),\n",
              " ('dia', 'inteiro'),\n",
              " ('inteiro', 'não'),\n",
              " ('não', 'sei'),\n",
              " ('sei', 'brinco'),\n",
              " ('brinco', 'sei'),\n",
              " ('sei', 'estudo'),\n",
              " ('estudo', 'saio'),\n",
              " ('saio', 'correndo'),\n",
              " ('correndo', 'fico'),\n",
              " ('fico', 'tranquilo'),\n",
              " ('tranquilo', 'mas'),\n",
              " ('mas', 'consegui'),\n",
              " ('consegui', 'entender'),\n",
              " ('entender', 'ainda'),\n",
              " ('ainda', 'melhor'),\n",
              " ('melhor', 'ou'),\n",
              " ('ou', 'isto'),\n",
              " ('isto', 'aquilo'),\n",
              " ('aquilo', 'cecília'),\n",
              " ('cecília', 'meireles')]"
            ]
          },
          "metadata": {},
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXysj_paJC2L",
        "outputId": "80ca97cc-f3f6-4414-94e1-eb1f8a06bcbe"
      },
      "source": [
        "list( trigrams(tokens_sem_stopwords) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('ou', 'chuva', 'sol'),\n",
              " ('chuva', 'sol', 'sol'),\n",
              " ('sol', 'sol', 'chuva'),\n",
              " ('sol', 'chuva', 'ou'),\n",
              " ('chuva', 'ou', 'calça'),\n",
              " ('ou', 'calça', 'luva'),\n",
              " ('calça', 'luva', 'põe'),\n",
              " ('luva', 'põe', 'anel'),\n",
              " ('põe', 'anel', 'põe'),\n",
              " ('anel', 'põe', 'anel'),\n",
              " ('põe', 'anel', 'calça'),\n",
              " ('anel', 'calça', 'luva'),\n",
              " ('calça', 'luva', 'quem'),\n",
              " ('luva', 'quem', 'sobe'),\n",
              " ('quem', 'sobe', 'ares'),\n",
              " ('sobe', 'ares', 'fica'),\n",
              " ('ares', 'fica', 'chão'),\n",
              " ('fica', 'chão', 'fica'),\n",
              " ('chão', 'fica', 'chão'),\n",
              " ('fica', 'chão', 'sobe'),\n",
              " ('chão', 'sobe', 'ares'),\n",
              " ('sobe', 'ares', 'grande'),\n",
              " ('ares', 'grande', 'pena'),\n",
              " ('grande', 'pena', 'possa'),\n",
              " ('pena', 'possa', 'estar'),\n",
              " ('possa', 'estar', 'tempo'),\n",
              " ('estar', 'tempo', 'dois'),\n",
              " ('tempo', 'dois', 'lugares'),\n",
              " ('dois', 'lugares', 'ou'),\n",
              " ('lugares', 'ou', 'guardo'),\n",
              " ('ou', 'guardo', 'dinheiro'),\n",
              " ('guardo', 'dinheiro', 'compro'),\n",
              " ('dinheiro', 'compro', 'doce'),\n",
              " ('compro', 'doce', 'compro'),\n",
              " ('doce', 'compro', 'doce'),\n",
              " ('compro', 'doce', 'gasto'),\n",
              " ('doce', 'gasto', 'dinheiro'),\n",
              " ('gasto', 'dinheiro', 'ou'),\n",
              " ('dinheiro', 'ou', 'vivo'),\n",
              " ('ou', 'vivo', 'escolhendo'),\n",
              " ('vivo', 'escolhendo', 'dia'),\n",
              " ('escolhendo', 'dia', 'inteiro'),\n",
              " ('dia', 'inteiro', 'não'),\n",
              " ('inteiro', 'não', 'sei'),\n",
              " ('não', 'sei', 'brinco'),\n",
              " ('sei', 'brinco', 'sei'),\n",
              " ('brinco', 'sei', 'estudo'),\n",
              " ('sei', 'estudo', 'saio'),\n",
              " ('estudo', 'saio', 'correndo'),\n",
              " ('saio', 'correndo', 'fico'),\n",
              " ('correndo', 'fico', 'tranquilo'),\n",
              " ('fico', 'tranquilo', 'mas'),\n",
              " ('tranquilo', 'mas', 'consegui'),\n",
              " ('mas', 'consegui', 'entender'),\n",
              " ('consegui', 'entender', 'ainda'),\n",
              " ('entender', 'ainda', 'melhor'),\n",
              " ('ainda', 'melhor', 'ou'),\n",
              " ('melhor', 'ou', 'isto'),\n",
              " ('ou', 'isto', 'aquilo'),\n",
              " ('isto', 'aquilo', 'cecília'),\n",
              " ('aquilo', 'cecília', 'meireles')]"
            ]
          },
          "metadata": {},
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPeh3-tuJIkj",
        "outputId": "606a3c98-ad7d-407c-eec2-dfbc92d30dfa"
      },
      "source": [
        "list( ngrams(tokens_sem_stopwords, 4) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('ou', 'chuva', 'sol', 'sol'),\n",
              " ('chuva', 'sol', 'sol', 'chuva'),\n",
              " ('sol', 'sol', 'chuva', 'ou'),\n",
              " ('sol', 'chuva', 'ou', 'calça'),\n",
              " ('chuva', 'ou', 'calça', 'luva'),\n",
              " ('ou', 'calça', 'luva', 'põe'),\n",
              " ('calça', 'luva', 'põe', 'anel'),\n",
              " ('luva', 'põe', 'anel', 'põe'),\n",
              " ('põe', 'anel', 'põe', 'anel'),\n",
              " ('anel', 'põe', 'anel', 'calça'),\n",
              " ('põe', 'anel', 'calça', 'luva'),\n",
              " ('anel', 'calça', 'luva', 'quem'),\n",
              " ('calça', 'luva', 'quem', 'sobe'),\n",
              " ('luva', 'quem', 'sobe', 'ares'),\n",
              " ('quem', 'sobe', 'ares', 'fica'),\n",
              " ('sobe', 'ares', 'fica', 'chão'),\n",
              " ('ares', 'fica', 'chão', 'fica'),\n",
              " ('fica', 'chão', 'fica', 'chão'),\n",
              " ('chão', 'fica', 'chão', 'sobe'),\n",
              " ('fica', 'chão', 'sobe', 'ares'),\n",
              " ('chão', 'sobe', 'ares', 'grande'),\n",
              " ('sobe', 'ares', 'grande', 'pena'),\n",
              " ('ares', 'grande', 'pena', 'possa'),\n",
              " ('grande', 'pena', 'possa', 'estar'),\n",
              " ('pena', 'possa', 'estar', 'tempo'),\n",
              " ('possa', 'estar', 'tempo', 'dois'),\n",
              " ('estar', 'tempo', 'dois', 'lugares'),\n",
              " ('tempo', 'dois', 'lugares', 'ou'),\n",
              " ('dois', 'lugares', 'ou', 'guardo'),\n",
              " ('lugares', 'ou', 'guardo', 'dinheiro'),\n",
              " ('ou', 'guardo', 'dinheiro', 'compro'),\n",
              " ('guardo', 'dinheiro', 'compro', 'doce'),\n",
              " ('dinheiro', 'compro', 'doce', 'compro'),\n",
              " ('compro', 'doce', 'compro', 'doce'),\n",
              " ('doce', 'compro', 'doce', 'gasto'),\n",
              " ('compro', 'doce', 'gasto', 'dinheiro'),\n",
              " ('doce', 'gasto', 'dinheiro', 'ou'),\n",
              " ('gasto', 'dinheiro', 'ou', 'vivo'),\n",
              " ('dinheiro', 'ou', 'vivo', 'escolhendo'),\n",
              " ('ou', 'vivo', 'escolhendo', 'dia'),\n",
              " ('vivo', 'escolhendo', 'dia', 'inteiro'),\n",
              " ('escolhendo', 'dia', 'inteiro', 'não'),\n",
              " ('dia', 'inteiro', 'não', 'sei'),\n",
              " ('inteiro', 'não', 'sei', 'brinco'),\n",
              " ('não', 'sei', 'brinco', 'sei'),\n",
              " ('sei', 'brinco', 'sei', 'estudo'),\n",
              " ('brinco', 'sei', 'estudo', 'saio'),\n",
              " ('sei', 'estudo', 'saio', 'correndo'),\n",
              " ('estudo', 'saio', 'correndo', 'fico'),\n",
              " ('saio', 'correndo', 'fico', 'tranquilo'),\n",
              " ('correndo', 'fico', 'tranquilo', 'mas'),\n",
              " ('fico', 'tranquilo', 'mas', 'consegui'),\n",
              " ('tranquilo', 'mas', 'consegui', 'entender'),\n",
              " ('mas', 'consegui', 'entender', 'ainda'),\n",
              " ('consegui', 'entender', 'ainda', 'melhor'),\n",
              " ('entender', 'ainda', 'melhor', 'ou'),\n",
              " ('ainda', 'melhor', 'ou', 'isto'),\n",
              " ('melhor', 'ou', 'isto', 'aquilo'),\n",
              " ('ou', 'isto', 'aquilo', 'cecília'),\n",
              " ('isto', 'aquilo', 'cecília', 'meireles')]"
            ]
          },
          "metadata": {},
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mo5auWTMJjEL"
      },
      "source": [
        "# STEMMING\n",
        "Consiste em reduzir a palavra ao seu radical.\n",
        "\n",
        "* amig: amigo, amiga, amigão\n",
        "* gat: gato, gata, gatos\n",
        "* prop: propõem, propuseram, propondo\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9TiGU27J4iE"
      },
      "source": [
        "## Vários algoritmos... o NLTK tem implementado vários algoritmos de\n",
        "para stemmer:\n",
        "\n",
        "* RSLP\n",
        "* Porter\n",
        "* ISRI\n",
        "* Lancaster\n",
        "* Snowball"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPdRirl1KJuD",
        "outputId": "09f7ddef-c797-4343-ef58-b99dd54cde6f"
      },
      "source": [
        "stemmer = nltk.RSLPStemmer() # RSLP = Removedor de Sufixos da Língua Portuguesa\n",
        "\n",
        "for w in ['amigão','amigos','amigas','amigável','propuseram','propondo','propôs','propõem']:\n",
        "  print( stemmer.stem(w) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "amig\n",
            "amig\n",
            "amig\n",
            "amig\n",
            "propus\n",
            "prop\n",
            "propô\n",
            "propõ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osvR73sfJtjr"
      },
      "source": [
        "# LEMATIZAÇÃO\n",
        "Consiste em reduzir a palavra à\n",
        "sua forma canônica, levando em conta sua\n",
        "classe gramatical.\n",
        "\n",
        "* propor: propõem, propuseram, propondo\n",
        "* estudar: estudando, estudioso, estudei"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BG-kWpYKzy7"
      },
      "source": [
        "Infelizmente o NLTK ainda não tem um\n",
        "lematizador para o Português bom o bastante.\n",
        "O WordNet Lemmatizer funciona para Inglês."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WMERlQ9LFQS",
        "outputId": "03813155-9fa4-4b79-c741-8ece423898b2"
      },
      "source": [
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "\n",
        "for w in ['studying','studied','sang','sings','singing']:\n",
        "  print( lemmatizer.lemmatize(w, pos='v') ) # try pos='n'\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "study\n",
            "study\n",
            "sing\n",
            "sing\n",
            "sing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wuc7eKu-MEPJ"
      },
      "source": [
        "# NLTK – Etiquetadores\n",
        " \n",
        "O NLTK possui dois corpus que servem como\n",
        "base para o etiquetador em português: o\n",
        "Floresta e o Mac Morpho.\n",
        "\n",
        "Para o inglês já existe um etiquetador padrão\n",
        "treinado: o `nltk.pos_tag()`.\n",
        "\n",
        "Os etiquetadores passam primeiramente por uma\n",
        "fase de treinamento com as sentenças presentes.\n",
        "* Floresta: 9.266 sentenças etiquetadas\n",
        "* Mac Morpho: 51.397 sentenças etiquetadas\n",
        "\n",
        "Como resultado, os etiquetadores retornam uma\n",
        "tupla (‘palavra’, ‘classe gramatical’), na qual a classe gramatical depende do treinamento\n",
        "que é realizado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXk0OvVfMSzb",
        "outputId": "e65147b4-e490-41e0-88cc-3527672aabca"
      },
      "source": [
        "from nltk.corpus import mac_morpho\n",
        "from nltk.tag import UnigramTagger\n",
        "\n",
        "sentencas_treinadoras = mac_morpho.tagged_sents()\n",
        "etiq = UnigramTagger(sentencas_treinadoras)\n",
        "\n",
        "tags = etiq.tag(tokens)\n",
        "tags\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Ou', 'KC'),\n",
              " ('se', 'PROPESS'),\n",
              " ('tem', 'V'),\n",
              " ('chuva', 'N'),\n",
              " ('e', 'KC'),\n",
              " ('não', 'ADV'),\n",
              " ('se', 'PROPESS'),\n",
              " ('tem', 'V'),\n",
              " ('sol', 'N'),\n",
              " ('ou', 'KC'),\n",
              " ('se', 'PROPESS'),\n",
              " ('tem', 'V'),\n",
              " ('sol', 'N'),\n",
              " ('e', 'KC'),\n",
              " ('não', 'ADV'),\n",
              " ('se', 'PROPESS'),\n",
              " ('tem', 'V'),\n",
              " ('chuva', 'N'),\n",
              " ('Ou', 'KC'),\n",
              " ('se', 'PROPESS'),\n",
              " ('calça', 'N'),\n",
              " ('a', 'ART'),\n",
              " ('luva', 'N'),\n",
              " ('e', 'KC'),\n",
              " ('não', 'ADV'),\n",
              " ('se', 'PROPESS'),\n",
              " ('põe', 'V'),\n",
              " ('o', 'ART'),\n",
              " ('anel', 'N'),\n",
              " ('ou', 'KC'),\n",
              " ('se', 'PROPESS'),\n",
              " ('põe', 'V'),\n",
              " ('o', 'ART'),\n",
              " ('anel', 'N'),\n",
              " ('e', 'KC'),\n",
              " ('não', 'ADV'),\n",
              " ('se', 'PROPESS'),\n",
              " ('calça', 'N'),\n",
              " ('a', 'ART'),\n",
              " ('luva', 'N'),\n",
              " ('Quem', 'PRO-KS'),\n",
              " ('sobe', 'V'),\n",
              " ('nos', 'PROPESS'),\n",
              " ('ares', 'N'),\n",
              " ('não', 'ADV'),\n",
              " ('fica', 'V'),\n",
              " ('no', 'KC'),\n",
              " ('chão', 'N'),\n",
              " ('quem', 'PRO-KS'),\n",
              " ('fica', 'V'),\n",
              " ('no', 'KC'),\n",
              " ('chão', 'N'),\n",
              " ('não', 'ADV'),\n",
              " ('sobe', 'V'),\n",
              " ('nos', 'PROPESS'),\n",
              " ('ares', 'N'),\n",
              " ('uma', 'ART'),\n",
              " ('grande', 'ADJ'),\n",
              " ('pena', 'N'),\n",
              " ('que', 'PRO-KS-REL'),\n",
              " ('não', 'ADV'),\n",
              " ('se', 'PROPESS'),\n",
              " ('possa', 'VAUX'),\n",
              " ('estar', 'V'),\n",
              " ('ao', 'PREP'),\n",
              " ('mesmo', 'PROADJ'),\n",
              " ('tempo', 'N'),\n",
              " ('nos', 'PROPESS'),\n",
              " ('dois', 'NUM'),\n",
              " ('lugares', 'N'),\n",
              " ('Ou', 'KC'),\n",
              " ('guardo', None),\n",
              " ('o', 'ART'),\n",
              " ('dinheiro', 'N'),\n",
              " ('e', 'KC'),\n",
              " ('não', 'ADV'),\n",
              " ('compro', 'V'),\n",
              " ('o', 'ART'),\n",
              " ('doce', 'ADJ'),\n",
              " ('ou', 'KC'),\n",
              " ('compro', 'V'),\n",
              " ('o', 'ART'),\n",
              " ('doce', 'ADJ'),\n",
              " ('e', 'KC'),\n",
              " ('gasto', 'N'),\n",
              " ('o', 'ART'),\n",
              " ('dinheiro', 'N'),\n",
              " ('Ou', 'KC'),\n",
              " ('isto', 'PROSUB'),\n",
              " ('ou', 'KC'),\n",
              " ('aquilo', 'PROSUB'),\n",
              " ('ou', 'KC'),\n",
              " ('isto', 'PROSUB'),\n",
              " ('ou', 'KC'),\n",
              " ('aquilo', 'PROSUB'),\n",
              " ('e', 'KC'),\n",
              " ('vivo', 'ADV'),\n",
              " ('escolhendo', 'V'),\n",
              " ('o', 'ART'),\n",
              " ('dia', 'N'),\n",
              " ('inteiro', 'ADJ'),\n",
              " ('Não', 'ADV'),\n",
              " ('sei', 'V'),\n",
              " ('se', 'PROPESS'),\n",
              " ('brinco', 'V'),\n",
              " ('não', 'ADV'),\n",
              " ('sei', 'V'),\n",
              " ('se', 'PROPESS'),\n",
              " ('estudo', 'N'),\n",
              " ('se', 'PROPESS'),\n",
              " ('saio', 'V'),\n",
              " ('correndo', 'V'),\n",
              " ('ou', 'KC'),\n",
              " ('fico', 'V'),\n",
              " ('tranquilo', 'ADJ'),\n",
              " ('Mas', 'KC'),\n",
              " ('não', 'ADV'),\n",
              " ('consegui', 'V'),\n",
              " ('entender', 'V'),\n",
              " ('ainda', 'ADV'),\n",
              " ('qual', 'PRO-KS-REL'),\n",
              " ('melhor', 'ADJ'),\n",
              " ('se', 'PROPESS'),\n",
              " ('isto', 'PROSUB'),\n",
              " ('ou', 'KC'),\n",
              " ('aquilo', 'PROSUB'),\n",
              " ('Ou', 'KC'),\n",
              " ('Isto', 'PROSUB'),\n",
              " ('ou', 'KC'),\n",
              " ('Aquilo', 'PROSUB'),\n",
              " ('Cecília', 'NPROP'),\n",
              " ('Meireles', 'NPROP')]"
            ]
          },
          "metadata": {},
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILtoubmpOGD8"
      },
      "source": [
        "# Análise Gramatical"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "nNq2y3tJOFmY",
        "outputId": "4a3855b2-2f78-441a-8967-7e5c4cf842ee"
      },
      "source": [
        "from nltk.chunk import RegexpParser\n",
        "pattern = 'NP:{<NPROP><NPROP>|<N><N>}'\n",
        "\n",
        "analiseGramatical = RegexpParser(pattern)\n",
        "arvore = analiseGramatical.parse(tags)\n",
        "\n",
        "print(arvore)\n",
        "arvore.draw()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-143-e97511b0670e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0manaliseGramatical\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRegexpParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0marvore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manaliseGramatical\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marvore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/chunk/regexp.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, chunk_struct, trace)\u001b[0m\n\u001b[1;32m   1206\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1207\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mparser\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1208\u001b[0;31m                 \u001b[0mchunk_struct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_struct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1209\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mchunk_struct\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/chunk/regexp.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, chunk_struct, trace)\u001b[0m\n\u001b[1;32m   1021\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrace\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1023\u001b[0;31m         \u001b[0mchunkstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mChunkString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_struct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1024\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m         \u001b[0;31m# Apply the sequence of rules to the chunkstring.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/chunk/regexp.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, chunk_struct, debug_level)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pieces\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchunk_struct\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mtags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtok\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtok\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pieces\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'<'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'><'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'>'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_debug\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdebug_level\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: sequence item 71: expected str instance, NoneType found"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-2yVnz1PbwM"
      },
      "source": [
        "## **DEU ERRO!** É isso mesmo continue..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_50RPahFPI1N",
        "outputId": "adf6a0fd-4bbf-4970-dd15-f219fc140201"
      },
      "source": [
        "tags[69:72]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('lugares', 'N'), ('Ou', 'KC'), ('guardo', None)]"
            ]
          },
          "metadata": {},
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfbGql_NPZ_0",
        "outputId": "2a601deb-1244-4503-8180-b4865317152b"
      },
      "source": [
        "from nltk.corpus import mac_morpho\n",
        "from nltk.tag import UnigramTagger\n",
        "from nltk.tag import DefaultTagger\n",
        "\n",
        "etiq_padrao = DefaultTagger('N')\n",
        "sentencas_treinadoras = mac_morpho.tagged_sents()\n",
        "etiq = UnigramTagger(sentencas_treinadoras, backoff=etiq_padrao)\n",
        "\n",
        "tags = etiq.tag(tokens)\n",
        "tags"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Ou', 'KC'),\n",
              " ('se', 'PROPESS'),\n",
              " ('tem', 'V'),\n",
              " ('chuva', 'N'),\n",
              " ('e', 'KC'),\n",
              " ('não', 'ADV'),\n",
              " ('se', 'PROPESS'),\n",
              " ('tem', 'V'),\n",
              " ('sol', 'N'),\n",
              " ('ou', 'KC'),\n",
              " ('se', 'PROPESS'),\n",
              " ('tem', 'V'),\n",
              " ('sol', 'N'),\n",
              " ('e', 'KC'),\n",
              " ('não', 'ADV'),\n",
              " ('se', 'PROPESS'),\n",
              " ('tem', 'V'),\n",
              " ('chuva', 'N'),\n",
              " ('Ou', 'KC'),\n",
              " ('se', 'PROPESS'),\n",
              " ('calça', 'N'),\n",
              " ('a', 'ART'),\n",
              " ('luva', 'N'),\n",
              " ('e', 'KC'),\n",
              " ('não', 'ADV'),\n",
              " ('se', 'PROPESS'),\n",
              " ('põe', 'V'),\n",
              " ('o', 'ART'),\n",
              " ('anel', 'N'),\n",
              " ('ou', 'KC'),\n",
              " ('se', 'PROPESS'),\n",
              " ('põe', 'V'),\n",
              " ('o', 'ART'),\n",
              " ('anel', 'N'),\n",
              " ('e', 'KC'),\n",
              " ('não', 'ADV'),\n",
              " ('se', 'PROPESS'),\n",
              " ('calça', 'N'),\n",
              " ('a', 'ART'),\n",
              " ('luva', 'N'),\n",
              " ('Quem', 'PRO-KS'),\n",
              " ('sobe', 'V'),\n",
              " ('nos', 'PROPESS'),\n",
              " ('ares', 'N'),\n",
              " ('não', 'ADV'),\n",
              " ('fica', 'V'),\n",
              " ('no', 'KC'),\n",
              " ('chão', 'N'),\n",
              " ('quem', 'PRO-KS'),\n",
              " ('fica', 'V'),\n",
              " ('no', 'KC'),\n",
              " ('chão', 'N'),\n",
              " ('não', 'ADV'),\n",
              " ('sobe', 'V'),\n",
              " ('nos', 'PROPESS'),\n",
              " ('ares', 'N'),\n",
              " ('uma', 'ART'),\n",
              " ('grande', 'ADJ'),\n",
              " ('pena', 'N'),\n",
              " ('que', 'PRO-KS-REL'),\n",
              " ('não', 'ADV'),\n",
              " ('se', 'PROPESS'),\n",
              " ('possa', 'VAUX'),\n",
              " ('estar', 'V'),\n",
              " ('ao', 'PREP'),\n",
              " ('mesmo', 'PROADJ'),\n",
              " ('tempo', 'N'),\n",
              " ('nos', 'PROPESS'),\n",
              " ('dois', 'NUM'),\n",
              " ('lugares', 'N'),\n",
              " ('Ou', 'KC'),\n",
              " ('guardo', 'N'),\n",
              " ('o', 'ART'),\n",
              " ('dinheiro', 'N'),\n",
              " ('e', 'KC'),\n",
              " ('não', 'ADV'),\n",
              " ('compro', 'V'),\n",
              " ('o', 'ART'),\n",
              " ('doce', 'ADJ'),\n",
              " ('ou', 'KC'),\n",
              " ('compro', 'V'),\n",
              " ('o', 'ART'),\n",
              " ('doce', 'ADJ'),\n",
              " ('e', 'KC'),\n",
              " ('gasto', 'N'),\n",
              " ('o', 'ART'),\n",
              " ('dinheiro', 'N'),\n",
              " ('Ou', 'KC'),\n",
              " ('isto', 'PROSUB'),\n",
              " ('ou', 'KC'),\n",
              " ('aquilo', 'PROSUB'),\n",
              " ('ou', 'KC'),\n",
              " ('isto', 'PROSUB'),\n",
              " ('ou', 'KC'),\n",
              " ('aquilo', 'PROSUB'),\n",
              " ('e', 'KC'),\n",
              " ('vivo', 'ADV'),\n",
              " ('escolhendo', 'V'),\n",
              " ('o', 'ART'),\n",
              " ('dia', 'N'),\n",
              " ('inteiro', 'ADJ'),\n",
              " ('Não', 'ADV'),\n",
              " ('sei', 'V'),\n",
              " ('se', 'PROPESS'),\n",
              " ('brinco', 'V'),\n",
              " ('não', 'ADV'),\n",
              " ('sei', 'V'),\n",
              " ('se', 'PROPESS'),\n",
              " ('estudo', 'N'),\n",
              " ('se', 'PROPESS'),\n",
              " ('saio', 'V'),\n",
              " ('correndo', 'V'),\n",
              " ('ou', 'KC'),\n",
              " ('fico', 'V'),\n",
              " ('tranquilo', 'ADJ'),\n",
              " ('Mas', 'KC'),\n",
              " ('não', 'ADV'),\n",
              " ('consegui', 'V'),\n",
              " ('entender', 'V'),\n",
              " ('ainda', 'ADV'),\n",
              " ('qual', 'PRO-KS-REL'),\n",
              " ('melhor', 'ADJ'),\n",
              " ('se', 'PROPESS'),\n",
              " ('isto', 'PROSUB'),\n",
              " ('ou', 'KC'),\n",
              " ('aquilo', 'PROSUB'),\n",
              " ('Ou', 'KC'),\n",
              " ('Isto', 'PROSUB'),\n",
              " ('ou', 'KC'),\n",
              " ('Aquilo', 'PROSUB'),\n",
              " ('Cecília', 'NPROP'),\n",
              " ('Meireles', 'NPROP')]"
            ]
          },
          "metadata": {},
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6Mb72ecP8Tl",
        "outputId": "ea8c4711-b164-4ada-c70a-88f31408c43f"
      },
      "source": [
        "from nltk.chunk import RegexpParser\n",
        "pattern = 'NP:{<NPROP><NPROP>|<N><N>}'\n",
        "\n",
        "analiseGramatical = RegexpParser(pattern)\n",
        "arvore = analiseGramatical.parse(tags)\n",
        "\n",
        "print(arvore)\n",
        "# arvore.draw() # not no Colab! :-()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  Ou/KC\n",
            "  se/PROPESS\n",
            "  tem/V\n",
            "  chuva/N\n",
            "  e/KC\n",
            "  não/ADV\n",
            "  se/PROPESS\n",
            "  tem/V\n",
            "  sol/N\n",
            "  ou/KC\n",
            "  se/PROPESS\n",
            "  tem/V\n",
            "  sol/N\n",
            "  e/KC\n",
            "  não/ADV\n",
            "  se/PROPESS\n",
            "  tem/V\n",
            "  chuva/N\n",
            "  Ou/KC\n",
            "  se/PROPESS\n",
            "  calça/N\n",
            "  a/ART\n",
            "  luva/N\n",
            "  e/KC\n",
            "  não/ADV\n",
            "  se/PROPESS\n",
            "  põe/V\n",
            "  o/ART\n",
            "  anel/N\n",
            "  ou/KC\n",
            "  se/PROPESS\n",
            "  põe/V\n",
            "  o/ART\n",
            "  anel/N\n",
            "  e/KC\n",
            "  não/ADV\n",
            "  se/PROPESS\n",
            "  calça/N\n",
            "  a/ART\n",
            "  luva/N\n",
            "  Quem/PRO-KS\n",
            "  sobe/V\n",
            "  nos/PROPESS\n",
            "  ares/N\n",
            "  não/ADV\n",
            "  fica/V\n",
            "  no/KC\n",
            "  chão/N\n",
            "  quem/PRO-KS\n",
            "  fica/V\n",
            "  no/KC\n",
            "  chão/N\n",
            "  não/ADV\n",
            "  sobe/V\n",
            "  nos/PROPESS\n",
            "  ares/N\n",
            "  uma/ART\n",
            "  grande/ADJ\n",
            "  pena/N\n",
            "  que/PRO-KS-REL\n",
            "  não/ADV\n",
            "  se/PROPESS\n",
            "  possa/VAUX\n",
            "  estar/V\n",
            "  ao/PREP\n",
            "  mesmo/PROADJ\n",
            "  tempo/N\n",
            "  nos/PROPESS\n",
            "  dois/NUM\n",
            "  lugares/N\n",
            "  Ou/KC\n",
            "  guardo/N\n",
            "  o/ART\n",
            "  dinheiro/N\n",
            "  e/KC\n",
            "  não/ADV\n",
            "  compro/V\n",
            "  o/ART\n",
            "  doce/ADJ\n",
            "  ou/KC\n",
            "  compro/V\n",
            "  o/ART\n",
            "  doce/ADJ\n",
            "  e/KC\n",
            "  gasto/N\n",
            "  o/ART\n",
            "  dinheiro/N\n",
            "  Ou/KC\n",
            "  isto/PROSUB\n",
            "  ou/KC\n",
            "  aquilo/PROSUB\n",
            "  ou/KC\n",
            "  isto/PROSUB\n",
            "  ou/KC\n",
            "  aquilo/PROSUB\n",
            "  e/KC\n",
            "  vivo/ADV\n",
            "  escolhendo/V\n",
            "  o/ART\n",
            "  dia/N\n",
            "  inteiro/ADJ\n",
            "  Não/ADV\n",
            "  sei/V\n",
            "  se/PROPESS\n",
            "  brinco/V\n",
            "  não/ADV\n",
            "  sei/V\n",
            "  se/PROPESS\n",
            "  estudo/N\n",
            "  se/PROPESS\n",
            "  saio/V\n",
            "  correndo/V\n",
            "  ou/KC\n",
            "  fico/V\n",
            "  tranquilo/ADJ\n",
            "  Mas/KC\n",
            "  não/ADV\n",
            "  consegui/V\n",
            "  entender/V\n",
            "  ainda/ADV\n",
            "  qual/PRO-KS-REL\n",
            "  melhor/ADJ\n",
            "  se/PROPESS\n",
            "  isto/PROSUB\n",
            "  ou/KC\n",
            "  aquilo/PROSUB\n",
            "  Ou/KC\n",
            "  Isto/PROSUB\n",
            "  ou/KC\n",
            "  Aquilo/PROSUB\n",
            "  (NP Cecília/NPROP Meireles/NPROP))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1-UtYzKRYmK"
      },
      "source": [
        "![imagem](https://miro.medium.com/max/700/1*mfcStHLTzMZC1evPaSJaag.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5HT4Nmt7-F2",
        "outputId": "16dd05cf-b644-4f0f-87b7-247102eb3c23"
      },
      "source": [
        "import spacy\n",
        "\n",
        "# Load English tokenizer, tagger, parser and NER\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Process whole documents\n",
        "text = (\"When Sebastian Thrun started working on self-driving cars at \"\n",
        "        \"Google in 2007, few people outside of the company took him \"\n",
        "        \"seriously. “I can tell you very senior CEOs of major American \"\n",
        "        \"car companies would shake my hand and turn away because I wasn’t \"\n",
        "        \"worth talking to,” said Thrun, in an interview with Recode earlier \"\n",
        "        \"this week.\")\n",
        "doc = nlp(text)\n",
        "\n",
        "# Analyze syntax\n",
        "print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])\n",
        "print(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])\n",
        "\n",
        "# Find named entities, phrases and concepts\n",
        "for entity in doc.ents:\n",
        "    print(entity.text, entity.label_)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Noun phrases: ['Sebastian Thrun', 'self-driving cars', 'Google', 'few people', 'the company', 'him', 'I', 'you', 'very senior CEOs', 'major American car companies', 'my hand', 'I', 'Thrun', 'an interview', 'Recode']\n",
            "Verbs: ['start', 'work', 'drive', 'take', 'can', 'tell', 'would', 'shake', 'turn', 'talk', 'say']\n",
            "Sebastian NORP\n",
            "Google ORG\n",
            "2007 DATE\n",
            "American NORP\n",
            "Recode ORG\n",
            "earlier this week DATE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJvSVePg8e4t",
        "outputId": "0f627e7b-4a74-47ac-83b3-36d6920681c2"
      },
      "source": [
        "print(\"Pontuation:\", [token.orth_ for token in doc if token.is_punct])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pontuation: ['-', ',', '.', '“', ',', '”', ',', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3YmgNvSN9QNe",
        "outputId": "a1b09102-964e-4d18-efa2-10627145585e"
      },
      "source": [
        "for token in doc:\n",
        "  print(token.pos_)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ADV\n",
            "PROPN\n",
            "PROPN\n",
            "VERB\n",
            "VERB\n",
            "ADP\n",
            "NOUN\n",
            "PUNCT\n",
            "VERB\n",
            "NOUN\n",
            "ADP\n",
            "PROPN\n",
            "ADP\n",
            "NUM\n",
            "PUNCT\n",
            "ADJ\n",
            "NOUN\n",
            "ADV\n",
            "ADP\n",
            "DET\n",
            "NOUN\n",
            "VERB\n",
            "PRON\n",
            "ADV\n",
            "PUNCT\n",
            "PUNCT\n",
            "PRON\n",
            "VERB\n",
            "VERB\n",
            "PRON\n",
            "ADV\n",
            "ADJ\n",
            "NOUN\n",
            "ADP\n",
            "ADJ\n",
            "ADJ\n",
            "NOUN\n",
            "NOUN\n",
            "VERB\n",
            "VERB\n",
            "DET\n",
            "NOUN\n",
            "CCONJ\n",
            "VERB\n",
            "ADV\n",
            "SCONJ\n",
            "PRON\n",
            "AUX\n",
            "PART\n",
            "ADJ\n",
            "VERB\n",
            "ADP\n",
            "PUNCT\n",
            "PUNCT\n",
            "VERB\n",
            "PROPN\n",
            "PUNCT\n",
            "ADP\n",
            "DET\n",
            "NOUN\n",
            "ADP\n",
            "PROPN\n",
            "ADV\n",
            "DET\n",
            "NOUN\n",
            "PUNCT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W49I4ed7FtPn"
      },
      "source": [
        "# Instalando a versão mais atual\n",
        "Empregamos o -U, o Colab não tem (ou pode não ter) a versão mais atual! Necessário fazer o **RESTART** do RUNTIME depois."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gJbzt2kOD_ty",
        "outputId": "517a7998-7dfa-484a-ca94-3f9b9eb499c0"
      },
      "source": [
        "!pip install -U spacy\n",
        "!pip install -U spacy-lookups-data \n",
        "\n",
        "!python -m spacy download pt_core_news_sm # sm small\n",
        "!python -m spacy download pt_core_news_md # md medium\n",
        "!python -m spacy download pt_core_news_lg # lg large!"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Collecting spacy\n",
            "  Downloading spacy-3.1.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.9 MB 8.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.62.3)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.7.4.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
            "Collecting typer<0.5.0,>=0.3.0\n",
            "  Downloading typer-0.4.0-py3-none-any.whl (27 kB)\n",
            "Collecting srsly<3.0.0,>=2.4.1\n",
            "  Downloading srsly-2.4.1-cp37-cp37m-manylinux2014_x86_64.whl (456 kB)\n",
            "\u001b[K     |████████████████████████████████| 456 kB 55.0 MB/s \n",
            "\u001b[?25hCollecting thinc<8.1.0,>=8.0.9\n",
            "  Downloading thinc-8.0.10-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (623 kB)\n",
            "\u001b[K     |████████████████████████████████| 623 kB 68.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (21.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
            "  Downloading pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 26.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Collecting catalogue<2.1.0,>=2.0.6\n",
            "  Downloading catalogue-2.0.6-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.5)\n",
            "Collecting pathy>=0.3.5\n",
            "  Downloading pathy-0.6.0-py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Collecting spacy-legacy<3.1.0,>=3.0.8\n",
            "  Downloading spacy_legacy-3.0.8-py2.py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy) (3.5.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (2.4.7)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.1)\n",
            "Installing collected packages: catalogue, typer, srsly, pydantic, thinc, spacy-legacy, pathy, spacy\n",
            "  Attempting uninstall: catalogue\n",
            "    Found existing installation: catalogue 1.0.0\n",
            "    Uninstalling catalogue-1.0.0:\n",
            "      Successfully uninstalled catalogue-1.0.0\n",
            "  Attempting uninstall: srsly\n",
            "    Found existing installation: srsly 1.0.5\n",
            "    Uninstalling srsly-1.0.5:\n",
            "      Successfully uninstalled srsly-1.0.5\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "Successfully installed catalogue-2.0.6 pathy-0.6.0 pydantic-1.8.2 spacy-3.1.3 spacy-legacy-3.0.8 srsly-2.4.1 thinc-8.0.10 typer-0.4.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "catalogue",
                  "spacy",
                  "srsly",
                  "thinc"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting spacy-lookups-data\n",
            "  Downloading spacy_lookups_data-1.0.2-py2.py3-none-any.whl (97.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 97.3 MB 77 kB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy-lookups-data) (57.4.0)\n",
            "Installing collected packages: spacy-lookups-data\n",
            "Successfully installed spacy-lookups-data-1.0.2\n",
            "Collecting pt-core-news-sm==3.1.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-3.1.0/pt_core_news_sm-3.1.0-py3-none-any.whl (21.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 21.9 MB 1.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.2.0,>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from pt-core-news-sm==3.1.0) (3.1.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->pt-core-news-sm==3.1.0) (21.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->pt-core-news-sm==3.1.0) (4.62.3)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->pt-core-news-sm==3.1.0) (0.6.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->pt-core-news-sm==3.1.0) (1.19.5)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->pt-core-news-sm==3.1.0) (0.4.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->pt-core-news-sm==3.1.0) (1.8.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->pt-core-news-sm==3.1.0) (1.0.5)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->pt-core-news-sm==3.1.0) (8.0.10)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->pt-core-news-sm==3.1.0) (3.7.4.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->pt-core-news-sm==3.1.0) (2.4.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->pt-core-news-sm==3.1.0) (0.8.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->pt-core-news-sm==3.1.0) (57.4.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->pt-core-news-sm==3.1.0) (3.0.8)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->pt-core-news-sm==3.1.0) (2.23.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->pt-core-news-sm==3.1.0) (2.11.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->pt-core-news-sm==3.1.0) (3.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->pt-core-news-sm==3.1.0) (2.0.5)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->pt-core-news-sm==3.1.0) (2.0.6)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->pt-core-news-sm==3.1.0) (0.4.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.2.0,>=3.1.0->pt-core-news-sm==3.1.0) (3.5.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.2.0,>=3.1.0->pt-core-news-sm==3.1.0) (2.4.7)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.2.0,>=3.1.0->pt-core-news-sm==3.1.0) (5.2.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->pt-core-news-sm==3.1.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->pt-core-news-sm==3.1.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->pt-core-news-sm==3.1.0) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->pt-core-news-sm==3.1.0) (2.10)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.2.0,>=3.1.0->pt-core-news-sm==3.1.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.2.0,>=3.1.0->pt-core-news-sm==3.1.0) (2.0.1)\n",
            "Installing collected packages: pt-core-news-sm\n",
            "Successfully installed pt-core-news-sm-3.1.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('pt_core_news_sm')\n",
            "Collecting pt-core-news-md==3.1.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_md-3.1.0/pt_core_news_md-3.1.0-py3-none-any.whl (50.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 50.9 MB 45 kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.2.0,>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from pt-core-news-md==3.1.0) (3.1.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->pt-core-news-md==3.1.0) (1.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->pt-core-news-md==3.1.0) (3.0.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->pt-core-news-md==3.1.0) (3.0.8)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->pt-core-news-md==3.1.0) (2.4.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->pt-core-news-md==3.1.0) (2.11.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->pt-core-news-md==3.1.0) (4.62.3)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->pt-core-news-md==3.1.0) (8.0.10)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->pt-core-news-md==3.1.0) (0.4.1)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->pt-core-news-md==3.1.0) (3.7.4.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->pt-core-news-md==3.1.0) (2.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->pt-core-news-md==3.1.0) (1.19.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->pt-core-news-md==3.1.0) (57.4.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->pt-core-news-md==3.1.0) (1.8.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->pt-core-news-md==3.1.0) (21.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->pt-core-news-md==3.1.0) (0.6.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->pt-core-news-md==3.1.0) (2.23.0)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->pt-core-news-md==3.1.0) (0.4.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->pt-core-news-md==3.1.0) (2.0.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->pt-core-news-md==3.1.0) (0.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.2.0,>=3.1.0->pt-core-news-md==3.1.0) (3.5.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.2.0,>=3.1.0->pt-core-news-md==3.1.0) (2.4.7)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.2.0,>=3.1.0->pt-core-news-md==3.1.0) (5.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->pt-core-news-md==3.1.0) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->pt-core-news-md==3.1.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->pt-core-news-md==3.1.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->pt-core-news-md==3.1.0) (3.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.2.0,>=3.1.0->pt-core-news-md==3.1.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.2.0,>=3.1.0->pt-core-news-md==3.1.0) (2.0.1)\n",
            "Installing collected packages: pt-core-news-md\n",
            "Successfully installed pt-core-news-md-3.1.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('pt_core_news_md')\n",
            "Collecting pt-core-news-lg==3.1.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_lg-3.1.0/pt_core_news_lg-3.1.0-py3-none-any.whl (576.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 576.7 MB 18 kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.2.0,>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from pt-core-news-lg==3.1.0) (3.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->pt-core-news-lg==3.1.0) (2.4.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->pt-core-news-lg==3.1.0) (2.0.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->pt-core-news-lg==3.1.0) (1.0.5)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->pt-core-news-lg==3.1.0) (0.6.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->pt-core-news-lg==3.1.0) (1.19.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->pt-core-news-lg==3.1.0) (0.8.2)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->pt-core-news-lg==3.1.0) (8.0.10)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->pt-core-news-lg==3.1.0) (2.23.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->pt-core-news-lg==3.1.0) (0.4.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->pt-core-news-lg==3.1.0) (3.0.8)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->pt-core-news-lg==3.1.0) (1.8.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->pt-core-news-lg==3.1.0) (2.11.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->pt-core-news-lg==3.1.0) (3.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->pt-core-news-lg==3.1.0) (4.62.3)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->pt-core-news-lg==3.1.0) (3.7.4.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->pt-core-news-lg==3.1.0) (2.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->pt-core-news-lg==3.1.0) (57.4.0)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->pt-core-news-lg==3.1.0) (0.4.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->pt-core-news-lg==3.1.0) (21.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.2.0,>=3.1.0->pt-core-news-lg==3.1.0) (3.5.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.2.0,>=3.1.0->pt-core-news-lg==3.1.0) (2.4.7)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.2.0,>=3.1.0->pt-core-news-lg==3.1.0) (5.2.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->pt-core-news-lg==3.1.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->pt-core-news-lg==3.1.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->pt-core-news-lg==3.1.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->pt-core-news-lg==3.1.0) (2021.5.30)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.2.0,>=3.1.0->pt-core-news-lg==3.1.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.2.0,>=3.1.0->pt-core-news-lg==3.1.0) (2.0.1)\n",
            "Installing collected packages: pt-core-news-lg\n",
            "Successfully installed pt-core-news-lg-3.1.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('pt_core_news_lg')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilRYDwxLHZKW"
      },
      "source": [
        "Veja aqui as acuracidades dos modelos de linguagem do spaCy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "caF4DiC5DN2H",
        "outputId": "df439cca-f0ab-4eeb-80d1-e154bacbd000"
      },
      "source": [
        "# Load English tokenizer, tagger, parser and NER\n",
        "import spacy\n",
        "import pt_core_news_lg\n",
        "\n",
        "nlp = spacy.load(\"pt_core_news_lg\")\n",
        "\n",
        "# Process whole documents\n",
        "text = ('Ou se tem chuva e não se tem sol,\\n' \n",
        " 'ou se tem sol e não se tem chuva!\\n' \n",
        " 'Ou se calça a luva e não se põe o anel,\\n' \n",
        " 'ou se põe o anel e não se calça a luva!\\n' \n",
        " 'Quem sobe nos ares não fica no chão,\\n' \n",
        " 'quem fica no chão não sobe nos ares.\\n' \n",
        " 'É uma grande pena que não se possa\\n' \n",
        " 'estar ao mesmo tempo nos dois lugares!\\n' \n",
        " 'Ou guardo o dinheiro e não compro o doce,\\n' \n",
        " 'ou compro o doce e gasto o dinheiro.\\n' \n",
        " 'Ou isto ou aquilo: ou isto ou aquilo…\\n' \n",
        " 'e vivo escolhendo o dia inteiro!\\n' \n",
        " 'Não sei se brinco, não sei se estudo,\\n' \n",
        " 'se saio correndo ou fico tranquilo.\\n' \n",
        " 'Mas não consegui entender ainda\\n' \n",
        " 'qual é melhor: se é isto ou aquilo.\\n' \n",
        " '\\n' \n",
        " '(Ou Isto ou Aquilo – Cecília Meireles)')\n",
        "\n",
        "doc = nlp(text)\n",
        "\n",
        "print(doc)\n",
        "# Analyze syntax\n",
        "# print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])\n",
        "# print(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])\n",
        "\n",
        "# Find named entities, phrases and concepts\n",
        "# for entity in doc.ents:\n",
        "#     print(entity.text, entity.label_)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ou se tem chuva e não se tem sol,\n",
            "ou se tem sol e não se tem chuva!\n",
            "Ou se calça a luva e não se põe o anel,\n",
            "ou se põe o anel e não se calça a luva!\n",
            "Quem sobe nos ares não fica no chão,\n",
            "quem fica no chão não sobe nos ares.\n",
            "É uma grande pena que não se possa\n",
            "estar ao mesmo tempo nos dois lugares!\n",
            "Ou guardo o dinheiro e não compro o doce,\n",
            "ou compro o doce e gasto o dinheiro.\n",
            "Ou isto ou aquilo: ou isto ou aquilo…\n",
            "e vivo escolhendo o dia inteiro!\n",
            "Não sei se brinco, não sei se estudo,\n",
            "se saio correndo ou fico tranquilo.\n",
            "Mas não consegui entender ainda\n",
            "qual é melhor: se é isto ou aquilo.\n",
            "\n",
            "(Ou Isto ou Aquilo – Cecília Meireles)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2CO3j9yHroG"
      },
      "source": [
        "# Tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMF_z9KgHrKn",
        "outputId": "4394d62e-de3f-4c66-cbd1-e5c77084f8f3"
      },
      "source": [
        "for token in doc:\n",
        "  print(token)\n",
        "\n",
        "# ou\n",
        "\n",
        "tokens = [token for token in doc]\n",
        "print(tokens)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ou\n",
            "se\n",
            "tem\n",
            "chuva\n",
            "e\n",
            "não\n",
            "se\n",
            "tem\n",
            "sol\n",
            ",\n",
            "\n",
            "\n",
            "ou\n",
            "se\n",
            "tem\n",
            "sol\n",
            "e\n",
            "não\n",
            "se\n",
            "tem\n",
            "chuva\n",
            "!\n",
            "\n",
            "\n",
            "Ou\n",
            "se\n",
            "calça\n",
            "a\n",
            "luva\n",
            "e\n",
            "não\n",
            "se\n",
            "põe\n",
            "o\n",
            "anel\n",
            ",\n",
            "\n",
            "\n",
            "ou\n",
            "se\n",
            "põe\n",
            "o\n",
            "anel\n",
            "e\n",
            "não\n",
            "se\n",
            "calça\n",
            "a\n",
            "luva\n",
            "!\n",
            "\n",
            "\n",
            "Quem\n",
            "sobe\n",
            "nos\n",
            "ares\n",
            "não\n",
            "fica\n",
            "no\n",
            "chão\n",
            ",\n",
            "\n",
            "\n",
            "quem\n",
            "fica\n",
            "no\n",
            "chão\n",
            "não\n",
            "sobe\n",
            "nos\n",
            "ares\n",
            ".\n",
            "\n",
            "\n",
            "É\n",
            "uma\n",
            "grande\n",
            "pena\n",
            "que\n",
            "não\n",
            "se\n",
            "possa\n",
            "\n",
            "\n",
            "estar\n",
            "ao\n",
            "mesmo\n",
            "tempo\n",
            "nos\n",
            "dois\n",
            "lugares\n",
            "!\n",
            "\n",
            "\n",
            "Ou\n",
            "guardo\n",
            "o\n",
            "dinheiro\n",
            "e\n",
            "não\n",
            "compro\n",
            "o\n",
            "doce\n",
            ",\n",
            "\n",
            "\n",
            "ou\n",
            "compro\n",
            "o\n",
            "doce\n",
            "e\n",
            "gasto\n",
            "o\n",
            "dinheiro\n",
            ".\n",
            "\n",
            "\n",
            "Ou\n",
            "isto\n",
            "ou\n",
            "aquilo\n",
            ":\n",
            "ou\n",
            "isto\n",
            "ou\n",
            "aquilo\n",
            "…\n",
            "\n",
            "\n",
            "e\n",
            "vivo\n",
            "escolhendo\n",
            "o\n",
            "dia\n",
            "inteiro\n",
            "!\n",
            "\n",
            "\n",
            "Não\n",
            "sei\n",
            "se\n",
            "brinco\n",
            ",\n",
            "não\n",
            "sei\n",
            "se\n",
            "estudo\n",
            ",\n",
            "\n",
            "\n",
            "se\n",
            "saio\n",
            "correndo\n",
            "ou\n",
            "fico\n",
            "tranquilo\n",
            ".\n",
            "\n",
            "\n",
            "Mas\n",
            "não\n",
            "consegui\n",
            "entender\n",
            "ainda\n",
            "\n",
            "\n",
            "qual\n",
            "é\n",
            "melhor\n",
            ":\n",
            "se\n",
            "é\n",
            "isto\n",
            "ou\n",
            "aquilo\n",
            ".\n",
            "\n",
            "\n",
            "\n",
            "(\n",
            "Ou\n",
            "Isto\n",
            "ou\n",
            "Aquilo\n",
            "–\n",
            "Cecília\n",
            "Meireles\n",
            ")\n",
            "[Ou, se, tem, chuva, e, não, se, tem, sol, ,, \n",
            ", ou, se, tem, sol, e, não, se, tem, chuva, !, \n",
            ", Ou, se, calça, a, luva, e, não, se, põe, o, anel, ,, \n",
            ", ou, se, põe, o, anel, e, não, se, calça, a, luva, !, \n",
            ", Quem, sobe, nos, ares, não, fica, no, chão, ,, \n",
            ", quem, fica, no, chão, não, sobe, nos, ares, ., \n",
            ", É, uma, grande, pena, que, não, se, possa, \n",
            ", estar, ao, mesmo, tempo, nos, dois, lugares, !, \n",
            ", Ou, guardo, o, dinheiro, e, não, compro, o, doce, ,, \n",
            ", ou, compro, o, doce, e, gasto, o, dinheiro, ., \n",
            ", Ou, isto, ou, aquilo, :, ou, isto, ou, aquilo, …, \n",
            ", e, vivo, escolhendo, o, dia, inteiro, !, \n",
            ", Não, sei, se, brinco, ,, não, sei, se, estudo, ,, \n",
            ", se, saio, correndo, ou, fico, tranquilo, ., \n",
            ", Mas, não, consegui, entender, ainda, \n",
            ", qual, é, melhor, :, se, é, isto, ou, aquilo, ., \n",
            "\n",
            ", (, Ou, Isto, ou, Aquilo, –, Cecília, Meireles, )]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nh4fxZesH-2_",
        "outputId": "f82a8a28-8e37-473e-bc07-2c769162eec4"
      },
      "source": [
        "tokens[0:10]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Ou, se, tem, chuva, e, não, se, tem, sol, ,]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIIfSvU1Iqmo"
      },
      "source": [
        "Esse tokens não são strings!!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OfOmesc8IvrI",
        "outputId": "923324a5-6f90-415a-c3e6-d8a3264d01b4"
      },
      "source": [
        "type( tokens[0] )"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "spacy.tokens.token.Token"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKAocU9pJDOw"
      },
      "source": [
        "Para isso vamos usar `.orth_ `"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jwokI-BHxwn",
        "outputId": "a7082a06-6a91-400d-b7ce-42d38995b95b"
      },
      "source": [
        "tokens = [token.orth_ for token in doc]\n",
        "print(tokens)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Ou', 'se', 'tem', 'chuva', 'e', 'não', 'se', 'tem', 'sol', ',', '\\n', 'ou', 'se', 'tem', 'sol', 'e', 'não', 'se', 'tem', 'chuva', '!', '\\n', 'Ou', 'se', 'calça', 'a', 'luva', 'e', 'não', 'se', 'põe', 'o', 'anel', ',', '\\n', 'ou', 'se', 'põe', 'o', 'anel', 'e', 'não', 'se', 'calça', 'a', 'luva', '!', '\\n', 'Quem', 'sobe', 'nos', 'ares', 'não', 'fica', 'no', 'chão', ',', '\\n', 'quem', 'fica', 'no', 'chão', 'não', 'sobe', 'nos', 'ares', '.', '\\n', 'É', 'uma', 'grande', 'pena', 'que', 'não', 'se', 'possa', '\\n', 'estar', 'ao', 'mesmo', 'tempo', 'nos', 'dois', 'lugares', '!', '\\n', 'Ou', 'guardo', 'o', 'dinheiro', 'e', 'não', 'compro', 'o', 'doce', ',', '\\n', 'ou', 'compro', 'o', 'doce', 'e', 'gasto', 'o', 'dinheiro', '.', '\\n', 'Ou', 'isto', 'ou', 'aquilo', ':', 'ou', 'isto', 'ou', 'aquilo', '…', '\\n', 'e', 'vivo', 'escolhendo', 'o', 'dia', 'inteiro', '!', '\\n', 'Não', 'sei', 'se', 'brinco', ',', 'não', 'sei', 'se', 'estudo', ',', '\\n', 'se', 'saio', 'correndo', 'ou', 'fico', 'tranquilo', '.', '\\n', 'Mas', 'não', 'consegui', 'entender', 'ainda', '\\n', 'qual', 'é', 'melhor', ':', 'se', 'é', 'isto', 'ou', 'aquilo', '.', '\\n\\n', '(', 'Ou', 'Isto', 'ou', 'Aquilo', '–', 'Cecília', 'Meireles', ')']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "valysMNsJJ7N",
        "outputId": "04388fba-1320-4014-9ec1-df7c45d0719f"
      },
      "source": [
        "type( tokens[0] )"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VC7LcI5vJJUg"
      },
      "source": [
        "# Extraindo alpha, dígitos e pontuação\n",
        "\n",
        "Veja outros atributos aqui."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wIkAf0YiJZAI",
        "outputId": "6bd2c305-ab2d-4f32-8eaf-bc5cd4f41936"
      },
      "source": [
        "tokens_alpha = [token.orth_ for token in doc if token.is_alpha]\n",
        "print(tokens_alpha)\n",
        "\n",
        "tokens_digit = [token.orth_ for token in doc if token.is_digit]\n",
        "print(tokens_digit)\n",
        "\n",
        "tokens_punct = [token.orth_ for token in doc if token.is_punct]\n",
        "print(tokens_punct)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Ou', 'se', 'tem', 'chuva', 'e', 'não', 'se', 'tem', 'sol', 'ou', 'se', 'tem', 'sol', 'e', 'não', 'se', 'tem', 'chuva', 'Ou', 'se', 'calça', 'a', 'luva', 'e', 'não', 'se', 'põe', 'o', 'anel', 'ou', 'se', 'põe', 'o', 'anel', 'e', 'não', 'se', 'calça', 'a', 'luva', 'Quem', 'sobe', 'nos', 'ares', 'não', 'fica', 'no', 'chão', 'quem', 'fica', 'no', 'chão', 'não', 'sobe', 'nos', 'ares', 'É', 'uma', 'grande', 'pena', 'que', 'não', 'se', 'possa', 'estar', 'ao', 'mesmo', 'tempo', 'nos', 'dois', 'lugares', 'Ou', 'guardo', 'o', 'dinheiro', 'e', 'não', 'compro', 'o', 'doce', 'ou', 'compro', 'o', 'doce', 'e', 'gasto', 'o', 'dinheiro', 'Ou', 'isto', 'ou', 'aquilo', 'ou', 'isto', 'ou', 'aquilo', 'e', 'vivo', 'escolhendo', 'o', 'dia', 'inteiro', 'Não', 'sei', 'se', 'brinco', 'não', 'sei', 'se', 'estudo', 'se', 'saio', 'correndo', 'ou', 'fico', 'tranquilo', 'Mas', 'não', 'consegui', 'entender', 'ainda', 'qual', 'é', 'melhor', 'se', 'é', 'isto', 'ou', 'aquilo', 'Ou', 'Isto', 'ou', 'Aquilo', 'Cecília', 'Meireles']\n",
            "[]\n",
            "[',', '!', ',', '!', ',', '.', '!', ',', '.', ':', '…', '!', ',', ',', '.', ':', '.', '(', '–', ')']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_h-xzDNTKC4Q"
      },
      "source": [
        "# Lemmatização\n",
        "\n",
        "O spaCy não tem stemming!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6BQT8eyKB_U",
        "outputId": "ab053c77-1f42-4794-c2e8-648ce174485f"
      },
      "source": [
        "# Analyze syntax\n",
        "print(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])\n",
        "\n",
        "# Lembre o texto aqui...\n",
        "#\n",
        "# Ou se TEM chuva e não se TEM sol,\n",
        "# ou se TEM sol e não se TEM chuva!\n",
        "# Ou se CALÇAR a luva e não se PÔR o anel,\n",
        "# ...\n",
        "\n",
        "lemmas_verbs = [token.lemma_ for token in doc if token.pos_ == \"VERB\"]"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verbs: ['ter', 'ter', 'ter', 'ter', 'calçar', 'pôr', 'pôr', 'calçar', 'subir', 'ficar', 'ficar', 'subir', 'poder', 'guardar', 'comprar', 'comprar', 'gastar', 'escolher', 'saber', 'saber', 'sair', 'correr', 'ficar', 'conseguir', 'entender']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLE6pG8fKvZx",
        "outputId": "5cb71d03-1bdc-440a-ccbf-5d0c95a76b45"
      },
      "source": [
        "lemmas_verbs"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ter',\n",
              " 'ter',\n",
              " 'ter',\n",
              " 'ter',\n",
              " 'calçar',\n",
              " 'pôr',\n",
              " 'pôr',\n",
              " 'calçar',\n",
              " 'subir',\n",
              " 'ficar',\n",
              " 'ficar',\n",
              " 'subir',\n",
              " 'poder',\n",
              " 'guardar',\n",
              " 'comprar',\n",
              " 'comprar',\n",
              " 'gastar',\n",
              " 'escolher',\n",
              " 'saber',\n",
              " 'saber',\n",
              " 'sair',\n",
              " 'correr',\n",
              " 'ficar',\n",
              " 'conseguir',\n",
              " 'entender']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCT8g7xKLBPD"
      },
      "source": [
        "# Etiquetador"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwniRC6ZLT1h",
        "outputId": "3c7efa6d-6778-480b-d091-96b762ffeebc"
      },
      "source": [
        "etiquetas = [ (token.orth_, token.morph) for token in doc]\n",
        "print(etiquetas)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Ou', ), ('se', Case=Acc|Gender=Unsp|PronType=Prs), ('tem', Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin), ('chuva', Gender=Fem|Number=Sing), ('e', ), ('não', Polarity=Neg), ('se', Case=Acc|Gender=Unsp|PronType=Prs), ('tem', Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin), ('sol', Gender=Masc|Number=Sing), (',', ), ('\\n', ), ('ou', ), ('se', Case=Acc|Gender=Fem|Number=Sing|Person=3|PronType=Prs), ('tem', Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin), ('sol', Gender=Masc|Number=Sing), ('e', ), ('não', Polarity=Neg), ('se', Case=Acc|Gender=Unsp|PronType=Prs), ('tem', Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin), ('chuva', Gender=Fem|Number=Sing), ('!', ), ('\\n', ), ('Ou', ), ('se', Case=Acc|Gender=Unsp|PronType=Prs), ('calça', Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin), ('a', Definite=Def|Gender=Fem|Number=Sing|PronType=Art), ('luva', Gender=Fem|Number=Sing), ('e', ), ('não', Polarity=Neg), ('se', Case=Acc|Gender=Unsp|PronType=Prs), ('põe', Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin), ('o', Definite=Def|Gender=Masc|Number=Sing|PronType=Art), ('anel', Gender=Masc|Number=Sing), (',', ), ('\\n', ), ('ou', ), ('se', Case=Acc|Gender=Fem|Number=Sing|Person=3|PronType=Prs), ('põe', Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin), ('o', Definite=Def|Gender=Masc|Number=Sing|PronType=Art), ('anel', Gender=Masc|Number=Sing), ('e', ), ('não', Polarity=Neg), ('se', Case=Acc|Gender=Unsp|PronType=Prs), ('calça', Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin), ('a', Definite=Def|Gender=Fem|Number=Sing|PronType=Art), ('luva', Gender=Fem|Number=Sing), ('!', ), ('\\n', ), ('Quem', Gender=Unsp|Number=Sing|PronType=Rel), ('sobe', Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin), ('nos', Definite=Def|Gender=Masc|Number=Plur|PronType=Art), ('ares', Gender=Masc|Number=Plur), ('não', Polarity=Neg), ('fica', Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin), ('no', Definite=Def|Gender=Masc|Number=Sing|PronType=Art), ('chão', Gender=Masc|Number=Sing), (',', ), ('\\n', ), ('quem', Gender=Masc|Number=Sing|PronType=Rel), ('fica', Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin), ('no', Definite=Def|Gender=Masc|Number=Sing|PronType=Art), ('chão', Gender=Masc|Number=Sing), ('não', Polarity=Neg), ('sobe', Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin), ('nos', Definite=Def|Gender=Masc|Number=Plur|PronType=Art), ('ares', Gender=Masc|Number=Plur), ('.', ), ('\\n', ), ('É', Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin), ('uma', Definite=Ind|Gender=Fem|Number=Sing|PronType=Art), ('grande', Gender=Fem|Number=Sing), ('pena', Gender=Fem|Number=Sing), ('que', Gender=Fem|Number=Sing|PronType=Rel), ('não', Polarity=Neg), ('se', Case=Acc|Gender=Unsp|PronType=Prs), ('possa', Mood=Sub|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin), ('\\n', ), ('estar', VerbForm=Inf), ('ao', Definite=Def|Gender=Masc|Number=Sing|PronType=Art), ('mesmo', Gender=Masc|Number=Sing), ('tempo', Gender=Masc|Number=Sing), ('nos', Definite=Def|Gender=Masc|Number=Plur|PronType=Art), ('dois', NumType=Card), ('lugares', Gender=Masc|Number=Plur), ('!', ), ('\\n', ), ('Ou', ), ('guardo', Mood=Ind|Number=Sing|Person=1|Tense=Pres|VerbForm=Fin), ('o', Definite=Def|Gender=Masc|Number=Sing|PronType=Art), ('dinheiro', Gender=Masc|Number=Sing), ('e', ), ('não', Polarity=Neg), ('compro', Mood=Ind|Number=Sing|Person=1|Tense=Pres|VerbForm=Fin), ('o', Definite=Def|Gender=Masc|Number=Sing|PronType=Art), ('doce', Gender=Masc|Number=Sing), (',', ), ('\\n', ), ('ou', ), ('compro', Mood=Ind|Number=Sing|Person=1|Tense=Pres|VerbForm=Fin), ('o', Definite=Def|Gender=Masc|Number=Sing|PronType=Art), ('doce', Gender=Masc|Number=Sing), ('e', ), ('gasto', Gender=Masc|Number=Sing|VerbForm=Part), ('o', Definite=Def|Gender=Masc|Number=Sing|PronType=Art), ('dinheiro', Gender=Masc|Number=Sing), ('.', ), ('\\n', ), ('Ou', ), ('isto', Gender=Masc|Number=Sing|PronType=Dem), ('ou', ), ('aquilo', Gender=Masc|Number=Sing|PronType=Dem), (':', ), ('ou', ), ('isto', Gender=Masc|Number=Sing|PronType=Dem), ('ou', ), ('aquilo', Gender=Masc|Number=Sing|PronType=Dem), ('…', ), ('\\n', ), ('e', ), ('vivo', Gender=Masc|Number=Sing), ('escolhendo', VerbForm=Ger), ('o', Definite=Def|Gender=Masc|Number=Sing|PronType=Art), ('dia', Gender=Masc|Number=Sing), ('inteiro', Gender=Masc|Number=Sing), ('!', ), ('\\n', ), ('Não', Polarity=Neg), ('sei', Mood=Ind|Number=Sing|Person=1|Tense=Past|VerbForm=Fin), ('se', ), ('brinco', Gender=Masc|Number=Sing), (',', ), ('não', Polarity=Neg), ('sei', Mood=Ind|Number=Sing|Person=1|Tense=Past|VerbForm=Fin), ('se', ), ('estudo', Gender=Masc|Number=Sing), (',', ), ('\\n', ), ('se', Case=Acc|Gender=Masc|Number=Sing|Person=3|PronType=Prs), ('saio', Mood=Ind|Number=Sing|Person=1|Tense=Pres|VerbForm=Fin), ('correndo', VerbForm=Ger), ('ou', ), ('fico', Mood=Ind|Number=Sing|Person=1|Tense=Pres|VerbForm=Fin), ('tranquilo', Gender=Masc|Number=Sing), ('.', ), ('\\n', ), ('Mas', ), ('não', Polarity=Neg), ('consegui', Mood=Ind|Number=Sing|Person=1|Tense=Past|VerbForm=Fin), ('entender', VerbForm=Inf), ('ainda', ), ('\\n', ), ('qual', Gender=Masc|Number=Sing|PronType=Rel), ('é', Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin), ('melhor', Gender=Masc|Number=Sing), (':', ), ('se', ), ('é', Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin), ('isto', Gender=Masc|Number=Sing|PronType=Dem), ('ou', ), ('aquilo', Gender=Masc|Number=Sing|PronType=Dem), ('.', ), ('\\n\\n', ), ('(', ), ('Ou', ), ('Isto', Gender=Masc|Number=Sing|PronType=Dem), ('ou', ), ('Aquilo', Gender=Fem|Number=Sing), ('–', ), ('Cecília', Gender=Fem|Number=Sing), ('Meireles', Number=Sing), (')', )]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t75f7ogoLrWY"
      },
      "source": [
        "# Entidades Nomeadas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CW2MGJjyLCw0",
        "outputId": "452faaad-9812-40fa-f700-a29823e49b08"
      },
      "source": [
        "for entity in doc.ents:\n",
        "    print(entity.text, entity.label_)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Aquilo LOC\n",
            "Cecília Meireles PER\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n76FLOjSL5gx"
      },
      "source": [
        "# displaCy\n",
        "\n",
        "Funciona, mas o exemplo fica melhor com mais entidades nomeadas! Veja o arquivo .html no diretório"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ljI965UNL8ax",
        "outputId": "01e707c1-929f-40c0-d3b4-cfe594fe05a3"
      },
      "source": [
        "from pathlib import Path\n",
        "\n",
        "html = spacy.displacy.render(doc, style='ent')\n",
        "output_path = Path('entidades_nomeadas.html')\n",
        "output_path.open('w', encoding='utf-8').write(html)\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1349"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eEdNBHsM9lj"
      },
      "source": [
        "# Análise Síntatica\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O99Vmvj0NEdK",
        "outputId": "425fe401-db79-4af4-ca5a-c8124e5b481c"
      },
      "source": [
        "sintatica = [ (token.orth_, token.dep_) for token in doc]\n",
        "print(sintatica)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Ou', 'cc'), ('se', 'nsubj'), ('tem', 'ROOT'), ('chuva', 'obj'), ('e', 'cc'), ('não', 'advmod'), ('se', 'nsubj'), ('tem', 'conj'), ('sol', 'obj'), (',', 'punct'), ('\\n', 'dep'), ('ou', 'cc'), ('se', 'nsubj'), ('tem', 'conj'), ('sol', 'obj'), ('e', 'cc'), ('não', 'advmod'), ('se', 'nsubj'), ('tem', 'conj'), ('chuva', 'obj'), ('!', 'punct'), ('\\n', 'mark'), ('Ou', 'cc'), ('se', 'expl'), ('calça', 'ROOT'), ('a', 'det'), ('luva', 'nsubj'), ('e', 'cc'), ('não', 'advmod'), ('se', 'nsubj'), ('põe', 'conj'), ('o', 'det'), ('anel', 'obj'), (',', 'punct'), ('\\n', 'dep'), ('ou', 'cc'), ('se', 'expl'), ('põe', 'conj'), ('o', 'det'), ('anel', 'obj'), ('e', 'cc'), ('não', 'advmod'), ('se', 'expl'), ('calça', 'conj'), ('a', 'det'), ('luva', 'nsubj'), ('!', 'punct'), ('\\n', 'mark'), ('Quem', 'nsubj'), ('sobe', 'advcl'), ('nos', 'case'), ('ares', 'obl'), ('não', 'advmod'), ('fica', 'conj'), ('no', 'case'), ('chão', 'obl'), (',', 'punct'), ('\\n', 'nsubj'), ('quem', 'nsubj'), ('fica', 'acl:relcl'), ('no', 'case'), ('chão', 'obl'), ('não', 'advmod'), ('sobe', 'ROOT'), ('nos', 'case'), ('ares', 'obl'), ('.', 'punct'), ('\\n', 'nsubj'), ('É', 'cop'), ('uma', 'det'), ('grande', 'amod'), ('pena', 'ROOT'), ('que', 'mark'), ('não', 'advmod'), ('se', 'nsubj'), ('possa', 'acl:relcl'), ('\\n', 'mark'), ('estar', 'cop'), ('ao', 'case'), ('mesmo', 'amod'), ('tempo', 'nmod'), ('nos', 'case'), ('dois', 'nummod'), ('lugares', 'nmod'), ('!', 'punct'), ('\\n', 'mark'), ('Ou', 'cc'), ('guardo', 'ROOT'), ('o', 'det'), ('dinheiro', 'obj'), ('e', 'cc'), ('não', 'advmod'), ('compro', 'conj'), ('o', 'det'), ('doce', 'obj'), (',', 'punct'), ('\\n', 'nsubj'), ('ou', 'cc'), ('compro', 'conj'), ('o', 'det'), ('doce', 'obj'), ('e', 'cc'), ('gasto', 'conj'), ('o', 'det'), ('dinheiro', 'obj'), ('.', 'punct'), ('\\n', 'mark'), ('Ou', 'cc'), ('isto', 'conj'), ('ou', 'cc'), ('aquilo', 'conj'), (':', 'punct'), ('ou', 'cc'), ('isto', 'conj'), ('ou', 'cc'), ('aquilo', 'conj'), ('…', 'punct'), ('\\n', 'nsubj'), ('e', 'cc'), ('vivo', 'conj'), ('escolhendo', 'ROOT'), ('o', 'det'), ('dia', 'obj'), ('inteiro', 'amod'), ('!', 'punct'), ('\\n', 'mark'), ('Não', 'advmod'), ('sei', 'ROOT'), ('se', 'mark'), ('brinco', 'ccomp'), (',', 'punct'), ('não', 'advmod'), ('sei', 'conj'), ('se', 'mark'), ('estudo', 'obj'), (',', 'punct'), ('\\n', 'nsubj'), ('se', 'expl'), ('saio', 'conj'), ('correndo', 'advcl'), ('ou', 'cc'), ('fico', 'conj'), ('tranquilo', 'xcomp'), ('.', 'punct'), ('\\n', 'mark'), ('Mas', 'cc'), ('não', 'advmod'), ('consegui', 'ROOT'), ('entender', 'xcomp'), ('ainda', 'advmod'), ('\\n', 'mark'), ('qual', 'nsubj'), ('é', 'cop'), ('melhor', 'ccomp'), (':', 'punct'), ('se', 'mark'), ('é', 'cop'), ('isto', 'advcl'), ('ou', 'cc'), ('aquilo', 'conj'), ('.', 'punct'), ('\\n\\n', 'ROOT'), ('(', 'punct'), ('Ou', 'cc'), ('Isto', 'parataxis'), ('ou', 'cc'), ('Aquilo', 'conj'), ('–', 'punct'), ('Cecília', 'conj'), ('Meireles', 'flat:name'), (')', 'punct')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jfy5nJFHNUwD",
        "outputId": "a431d09b-59c7-4b44-a8ad-dce25f6c76b6"
      },
      "source": [
        "html = spacy.displacy.render(doc, style='dep')\n",
        "output_path = Path('analise_dependencia.html')\n",
        "output_path.open('w', encoding='utf-8').write(html)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "122306"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pldusWmsNqBB"
      },
      "source": [
        "O spaCy contém dois sites onde podem ser feitas\n",
        "as análises de entidades nomeadas e de\n",
        "dependências de forma bem simples:\n",
        "\n",
        "* Visualizador de Entidades Nomeadas\n",
        "https://explosion.ai/demos/displacy-ent\n",
        "* Visualizador de Dependências\n",
        "https://explosion.ai/demos/displacy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amqDoaUUOEN8"
      },
      "source": [
        "# Similaridade de Palavras\n",
        "\n",
        "A similaridade emprega a distância cosseno!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TA0YUKvrOHKS",
        "outputId": "375910e7-b8f8-4125-991d-7e2f8e64b6c8"
      },
      "source": [
        "texto = 'Brazil Europa América Argentina França Rogério Adriana Daniel Henrique amar escalar sorrir ensinar'\n",
        "\n",
        "doc = nlp(texto)\n",
        "\n",
        "tokens = [token for token in doc]\n",
        "tokens"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Brazil,\n",
              " Europa,\n",
              " América,\n",
              " Argentina,\n",
              " França,\n",
              " Rogério,\n",
              " Adriana,\n",
              " Daniel,\n",
              " Henrique,\n",
              " amar,\n",
              " escalar,\n",
              " sorrir,\n",
              " ensinar]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1h2i-iLOuhW",
        "outputId": "d96bbdfe-e7d8-4787-be10-8af38107b16e"
      },
      "source": [
        "for t in tokens:\n",
        "  print(tokens[0], t, tokens[0].similarity(t))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Brazil Brazil 1.0\n",
            "Brazil Europa 0.14360705\n",
            "Brazil América 0.3213451\n",
            "Brazil Argentina 0.23673028\n",
            "Brazil França 0.18687184\n",
            "Brazil Rogério 0.085951\n",
            "Brazil Adriana 0.14536843\n",
            "Brazil Daniel 0.09794333\n",
            "Brazil Henrique 0.14294225\n",
            "Brazil amar -0.19550382\n",
            "Brazil escalar -0.050032772\n",
            "Brazil sorrir -0.14495963\n",
            "Brazil ensinar -0.10491401\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PwtS3hkgPZmL",
        "outputId": "5e917b41-e6d1-4267-8344-e61b32c0d86a"
      },
      "source": [
        "for t in tokens:\n",
        "  print(tokens[5], t, tokens[5].similarity(t))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rogério Brazil 0.085951\n",
            "Rogério Europa -0.07335681\n",
            "Rogério América 0.011733616\n",
            "Rogério Argentina 0.118416116\n",
            "Rogério França 0.2023491\n",
            "Rogério Rogério 1.0\n",
            "Rogério Adriana 0.59579444\n",
            "Rogério Daniel 0.6294814\n",
            "Rogério Henrique 0.6444034\n",
            "Rogério amar -0.06835476\n",
            "Rogério escalar 0.06600208\n",
            "Rogério sorrir -0.063787736\n",
            "Rogério ensinar -0.044531334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLULPpcVP2Fm"
      },
      "source": [
        "# Word2Vec\n",
        "\n",
        "Ver aqui... https://cursosextensao.usp.br/pluginfile.php/753932/mod_resource/content/0/Aula%205%20-%20material%20extra.pdf[link text](https://)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8n2QqmjC0esQ"
      },
      "source": [
        "# Word2Vec Application Tutorial\n",
        "In this tutorial, we go over basic operations on word vectors. There are many Natural Language Processing (NLP) libraries in Python, such as [NLTK](https://www.nltk.org/), [gensim](https://radimrehurek.com/gensim/), and [spaCy](https://spacy.io/). All of them have their own strength and focus. NLTK is one of the first comprehensive Python libraries for computational linguistics and has a big community. If you have worked on NLP, you probably have heard of it or used it. Gensim is a popular library for topic modeling. It also provides many functionalities similar to NLTK. It supports word embeddings and you can even train word embeddings using gensim. SpaCy is another popular NLP library and it provides built-in support for word vectors. We will use spaCy in this tutorial.  \\\\\n",
        "<br>\n",
        "You will learn:\n",
        "\n",
        "\n",
        "1.   Popular Python machine learning packages (spaCy, sklearn)\n",
        "2.   Calculating word similarity using Word2Vec model\n",
        "3.   Word analogy analysis\n",
        "4.   Calculating sentence similarity using Word2Vec model\n",
        "5.   Dimension reduction techniques for high-dimensional vectors\n",
        "6.   Visualizing Word2Vec in 2D space\n",
        "7.   Sentiment analysis using logistic regression and Word2Vec\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fymQg_Q068-"
      },
      "source": [
        "### Preliminary\n",
        "First, let's install the spaCy Python library and download their model for the English language. We only need to do it once. Then we can import the spaCy library and other useful libraries such as numpy (used for linear algebra and vector operations in Python). We can load our downloaded English model in our environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VJCepyC9ZbX"
      },
      "source": [
        "# Only needs to be run once at the top of the notebook\n",
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_lg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzPVan-OAIw8"
      },
      "source": [
        "# import packages\n",
        "import spacy\n",
        "import numpy as np\n",
        "import csv\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn import linear_model\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from IPython.display import clear_output"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4tTrKWvQeid",
        "outputId": "43a28f78-9599-4fad-fe48-f8e733c4f345"
      },
      "source": [
        "!python -m spacy download en_core_web_lg"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-lg==3.1.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.1.0/en_core_web_lg-3.1.0-py3-none-any.whl (777.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 777.1 MB 13 kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.2.0,>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-lg==3.1.0) (3.1.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-lg==3.1.0) (4.62.3)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-lg==3.1.0) (3.7.4.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-lg==3.1.0) (3.0.8)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-lg==3.1.0) (1.8.2)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-lg==3.1.0) (0.6.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-lg==3.1.0) (2.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-lg==3.1.0) (2.23.0)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-lg==3.1.0) (0.4.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-lg==3.1.0) (1.19.5)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-lg==3.1.0) (8.0.10)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-lg==3.1.0) (3.0.5)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-lg==3.1.0) (2.0.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-lg==3.1.0) (2.11.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-lg==3.1.0) (2.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-lg==3.1.0) (57.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-lg==3.1.0) (1.0.5)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-lg==3.1.0) (0.4.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-lg==3.1.0) (21.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-lg==3.1.0) (0.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.2.0,>=3.1.0->en-core-web-lg==3.1.0) (3.5.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.2.0,>=3.1.0->en-core-web-lg==3.1.0) (2.4.7)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.2.0,>=3.1.0->en-core-web-lg==3.1.0) (5.2.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-lg==3.1.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-lg==3.1.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-lg==3.1.0) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-lg==3.1.0) (1.24.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.2.0,>=3.1.0->en-core-web-lg==3.1.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.2.0,>=3.1.0->en-core-web-lg==3.1.0) (2.0.1)\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-3.1.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9wiQz4BAuQT"
      },
      "source": [
        "nlp = spacy.load('en_core_web_lg')  # load the English model"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfpycYEG2T3y"
      },
      "source": [
        "### Word Similarity\n",
        "By representing words in vectors, we can use linear algebra and vector space models to analyze the relationship between words. One simple task is to calculate the cosine of two word vectors, namely the cosine similarity. This cosine similarity measures the semantic similarity of words. While the value ranges from -1 to 1, it is usually used in the non-negative space [0, 1] where 0 means 0 similarity and 1 means extremely similar or even identical. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qm-5ZI0gHp-w"
      },
      "source": [
        "In order to calculate the cosine similarity between words, we have to know their vector representations first, which are provided by the Word2Vec model. In the spaCy English model, these vector representations (pretrained using Word2Vec) are already provided. All we need to do is to retrieve these words from the spaCy English model and we will have access to these vector representations. \\\\\n",
        "\n",
        "<br>\n",
        "\n",
        "![cosine_sim](https://engineering.aweber.com/wp-content/uploads/2013/02/4AUbj.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtM-gBpvQ7hd"
      },
      "source": [
        "## Uma representação em um vetor de dimensão 300"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-HL5bHMA3RE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "334ff454-04f6-4117-d852-5698589b177a"
      },
      "source": [
        "# retrieve words from the English model vocabulary\n",
        "cat = nlp.vocab['cat']\n",
        "dog = nlp.vocab['dog']\n",
        "car = nlp.vocab['car']\n",
        "\n",
        "# print the dimension of word vectors\n",
        "print('vector length:', len(cat.vector))\n",
        "\n",
        "# print the word vector\n",
        "print('cat:', cat.vector)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vector length: 300\n",
            "cat: [-0.15067   -0.024468  -0.23368   -0.23378   -0.18382    0.32711\n",
            " -0.22084   -0.28777    0.12759    1.1656    -0.64163   -0.098455\n",
            " -0.62397    0.010431  -0.25653    0.31799    0.037779   1.1904\n",
            " -0.17714   -0.2595    -0.31461    0.038825  -0.15713   -0.13484\n",
            "  0.36936   -0.30562   -0.40619   -0.38965    0.3686     0.013963\n",
            " -0.6895     0.004066  -0.1367     0.32564    0.24688   -0.14011\n",
            "  0.53889   -0.80441   -0.1777    -0.12922    0.16303    0.14917\n",
            " -0.068429  -0.33922    0.18495   -0.082544  -0.46892    0.39581\n",
            " -0.13742   -0.35132    0.22223   -0.144     -0.048287   0.3379\n",
            " -0.31916    0.20526    0.098624  -0.23877    0.045338   0.43941\n",
            "  0.030385  -0.013821  -0.093273  -0.18178    0.19438   -0.3782\n",
            "  0.70144    0.16236    0.0059111  0.024898  -0.13613   -0.11425\n",
            " -0.31598   -0.14209    0.028194   0.5419    -0.42413   -0.599\n",
            "  0.24976   -0.27003    0.14964    0.29287   -0.31281    0.16543\n",
            " -0.21045   -0.4408     1.2174     0.51236    0.56209    0.14131\n",
            "  0.092514   0.71396   -0.021051  -0.33704   -0.20275   -0.36181\n",
            "  0.22055   -0.25665    0.28425   -0.16968    0.058029   0.61182\n",
            "  0.31576   -0.079185   0.35538   -0.51236    0.4235    -0.30033\n",
            " -0.22376    0.15223   -0.048292   0.23532    0.46507   -0.67579\n",
            " -0.32905    0.08446   -0.22123   -0.045333   0.34463   -0.1455\n",
            " -0.18047   -0.17887    0.96879   -1.0028    -0.47343    0.28542\n",
            "  0.56382   -0.33211   -0.38275   -0.2749    -0.22955   -0.24265\n",
            " -0.37689    0.24822    0.36941    0.14651   -0.37864    0.31134\n",
            " -0.28449    0.36948   -2.8174    -0.38319   -0.022373   0.56376\n",
            "  0.40131   -0.42131   -0.11311   -0.17317    0.1411    -0.13194\n",
            "  0.18494    0.097692  -0.097341  -0.23987    0.16631   -0.28556\n",
            "  0.0038654  0.53292   -0.32367   -0.38744    0.27011   -0.34181\n",
            " -0.27702   -0.67279   -0.10771   -0.062189  -0.24783   -0.070884\n",
            " -0.20898    0.062404   0.022372   0.13408    0.1305    -0.19546\n",
            " -0.46849    0.77731   -0.043978   0.3827    -0.23376    1.0457\n",
            " -0.14371   -0.3565    -0.080713  -0.31047   -0.57822   -0.28067\n",
            " -0.069678   0.068929  -0.16227   -0.63934   -0.62149    0.11222\n",
            " -0.16969   -0.54637    0.49661    0.46565    0.088294  -0.48496\n",
            "  0.69263   -0.068977  -0.53709    0.20802   -0.42987   -0.11921\n",
            "  0.1174    -0.18443    0.43797   -0.1236     0.3607    -0.19608\n",
            " -0.35366    0.18808   -0.5061     0.14455   -0.024368  -0.10772\n",
            " -0.0115     0.58634   -0.054461   0.0076487 -0.056297   0.27193\n",
            "  0.23096   -0.29296   -0.24325    0.10317   -0.10014    0.7089\n",
            "  0.17402   -0.0037509 -0.46304    0.11806   -0.16457   -0.38609\n",
            "  0.14524    0.098122  -0.12352   -0.1047     0.39047   -0.3063\n",
            " -0.65375   -0.0044248 -0.033876   0.037114  -0.27472    0.0053147\n",
            "  0.30737    0.12528   -0.19527   -0.16461    0.087518  -0.051107\n",
            " -0.16323    0.521      0.10822   -0.060379  -0.71735   -0.064327\n",
            "  0.37043   -0.41054   -0.2728    -0.30217    0.015771  -0.43056\n",
            "  0.35647    0.17188   -0.54598   -0.21541   -0.044889  -0.10597\n",
            " -0.54391    0.53908    0.070938   0.097839   0.097908   0.17805\n",
            "  0.18995    0.49962   -0.18529    0.051234   0.019574   0.24805\n",
            "  0.3144    -0.29304    0.54235    0.46672    0.26017   -0.44705\n",
            "  0.28287   -0.033345  -0.33181   -0.10902   -0.023324   0.2106\n",
            " -0.29633    0.81506    0.038524   0.46004    0.17187   -0.29804  ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmpDwypeHSIy"
      },
      "source": [
        "Try to retrieve some other words and check if they have the same dimension."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NO5S4yX9hgZg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56ce77d3-c29f-4a6d-def2-27df2b00053d"
      },
      "source": [
        "# try your own words and check if they have the same dimension of the cat vector\n",
        "############# YOUR CODE HERE ################\n",
        "computer = nlp.vocab['computer']\n",
        "\n",
        "print(len(computer.vector))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-g5mO0AfIT94"
      },
      "source": [
        "After retrieving the words and their vector representations, we can use the built-in similarity function (which implements cosine similarity) to calculate word similarity based on these vectors. Is 'cat' more similar to 'dog' than 'car'? Can you find some properties of cosine similarity?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpxnslv1DB7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d41de5e-39f8-4387-f313-88ad5ff5995c"
      },
      "source": [
        "# you can calculate the similarity between words using \n",
        "# the built-in 'similarity' function\n",
        "print('The similarity between cat and cat:', cat.similarity(cat))\n",
        "print('The similarity between cat and dog:', cat.similarity(dog))\n",
        "print('The similarity between dog and cat:', dog.similarity(cat))\n",
        "print('The similarity between cat and car:', cat.similarity(car))\n",
        "print('The similarity between dog and car:', dog.similarity(car))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The similarity between cat and cat: 1.0\n",
            "The similarity between cat and dog: 0.80168545\n",
            "The similarity between dog and cat: 0.80168545\n",
            "The similarity between cat and car: 0.31907532\n",
            "The similarity between dog and car: 0.3562916\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pe1G8CvuIxIP"
      },
      "source": [
        "Now let's try some other words. Also, try to calculate the cosine similarity between 'hotel' and 'motel' and the cosine similarity between 'hotel' and 'hospital'. Which one is more similar to 'hotel'? 'motel' or 'hospital'?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QALZbd0Jhjlf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a16d956-4520-4a27-d4ae-ab37973b7efe"
      },
      "source": [
        "# calculate the similarity of your own words using the built-in function\n",
        "############# YOUR CODE HERE ################\n",
        "hotel = nlp.vocab['hotel']\n",
        "motel = nlp.vocab['motel']\n",
        "hospital = nlp.vocab['hospital']\n",
        "\n",
        "# what is the similarity between (hotel, motel) and (hotel, hospital)\n",
        "############# YOUR CODE HERE ################\n",
        "print(hotel.similarity(motel))\n",
        "\n",
        "print(hotel.similarity(hospital))\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.74046886\n",
            "0.3863955\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jwt8kG5Kuof"
      },
      "source": [
        "Let's compute the cosine similarity manually using its definition below. Then check if the result is the same as the one calculated by the built-in function. \\\\\n",
        "<br>\n",
        "$cosine\\_similarity(A, B) = \\frac{A \\cdot B}{\\left \\| A \\right \\|\\left \\| B \\right \\|}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3ZzlyeLhm3e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08de688b-d793-4ea9-a1e2-3fc4ca86c084"
      },
      "source": [
        "# try to calculate cosine similarity manually\n",
        "'''\n",
        "cosine of V1 and V2 = dot product of V1 and V2 / product of V1 norm and V2 norm\n",
        "To get the vector representation of a word, use .vector, e.g. car.vector\n",
        "To calculate the dot product of two vectors V1 and V2, use np.dot(V1, V2)\n",
        "To get the norm of a word vector, use .vector_norm, e.g. car.vector_norm, \n",
        "alternatively you can use np.linalg.norm(V1) to calculate the norm of V1\n",
        "'''\n",
        "############# YOUR CODE HERE ################\n",
        "cosine_dog_car = np.dot(dog.vector, car.vector)/(dog.vector_norm*car.vector_norm)\n",
        "print('The similarity between dog and car calculated manually:', cosine_dog_car)\n",
        "#############################################"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The similarity between dog and car calculated manually: 0.3562916\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ma4PgQeaMZfM"
      },
      "source": [
        "Now we know how to compare the similarity of two words using pretrained Word2Vec model. We can also use it to help us find semantically similar words, that is given a word retrieve similar words from the vocabulary. \\\\\n",
        "<br>\n",
        "The Python spaCy library hasn't provided such a function to do precisely this yet. We could use other NLP and machine learning libraries, such as [gensim](https://radimrehurek.com/gensim/), to do this with a simple function call. But the implementation is not hard, so let's give it a try! In our customized function, we first find all the words in our vocabulary (that has vector representations). Then we calculate the cosine similarity between our query word and each word in the vocabulary. We sort the similarity score in descending order. Finally, we retrieve the top n most similar words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6dRHeE6KBq5"
      },
      "source": [
        "# function to find similar words\n",
        "def most_similar(word, topn=10):\n",
        "    allwords = [w for w in nlp.vocab if w.has_vector and w.is_lower and w.lower_ != word.lower_]  # get all words in the vocabulary\n",
        "    by_similarity = sorted(allwords, key=lambda w: word.similarity(w), reverse=True)  # sort words by similarity in descending order\n",
        "    return by_similarity[:topn]"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCemJQbROBpi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4006ea58-e691-4f56-8d6a-ece99aa9820d"
      },
      "source": [
        "# find similar words\n",
        "cat_similar = [w.text for w in most_similar(dog)]\n",
        "print('Similar words to cat: ', cat_similar)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similar words to cat:  ['cat', 'she', 'when', 'he', 'how', 'does', 'let', 'that', 'what', 'you']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYclgBII2gc7"
      },
      "source": [
        "### Word Analogy\n",
        "One interesting finding for the Word2Vec model is that it embeds some analogical relationships between words. \\\\\n",
        "<br>\n",
        "*Man is to Woman as King is to Queen* \\\\\n",
        "Man - Woman = King - Queen \\\\\n",
        "<br>\n",
        "*Paris is to France as Madrid is to Spain* \\\\\n",
        "Paris - France = Madrid - Spain \\\\\n",
        "<br>\n",
        "These relationships can be reconstructed using word embeddings. \\\\\n",
        "<br>\n",
        "![analogy](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/06/06062705/Word-Vectors.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6Xo9Prcfpoy"
      },
      "source": [
        "# word analogy example\n",
        "# king is to man as what is to woman?\n",
        "king = nlp.vocab['king']\n",
        "man = nlp.vocab['man']\n",
        "woman = nlp.vocab['woman']\n",
        "\n",
        "# resulting vector\n",
        "result = king.vector - man.vector + woman.vector\n",
        "\n",
        "# function to compute cosine similarity\n",
        "cosine = lambda v1, v2: np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vg_R5y2MyWFv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcd814c9-956b-4d72-c374-2aa02345c0d7"
      },
      "source": [
        "# what word does the 'result' vector closely correspond to?\n",
        "\n",
        "# we can first check if the 'result' vector is similar to the 'queen' vector\n",
        "############# YOUR CODE HERE ################\n",
        "queen = nlp.vocab['queen']\n",
        "print('Similarity between queen and result:', cosine(result, queen.vector))\n",
        "#############################################\n",
        "\n",
        "# find all words in our vocabulary (nlp.vocab), \n",
        "# make sure to just retrieve lower case words \n",
        "# and words that actually have vectors (.has_vector) \n",
        "# and filter out 'king', 'man', and 'woman'\n",
        "############# YOUR CODE HERE ################\n",
        "allwords = [w for w in nlp.vocab if w.has_vector and w.is_lower and w.lower_ != 'king' and w.lower_ != 'man' and w.lower_ != 'woman']\n",
        "#############################################\n",
        "\n",
        "# calculate the cosine similarity between the 'result' vector \n",
        "# and all word vectors in our vocabulary\n",
        "# sort by similarity and print out the most similar one\n",
        "############# YOUR CODE HERE ################\n",
        "candidates = sorted(allwords, key=lambda w: cosine(result, w.vector), reverse=True)\n",
        "print([c.text for c in candidates[:5]])"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity between queen and result: 0.78808445\n",
            "['queen', 'she', 'who', 'r.', 'when']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHDhFaO2VaFJ"
      },
      "source": [
        "Let's try: \\\\\n",
        "Paris - France = Madrid - Spain"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unaXunKv0Y0F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46d07b61-3376-45c9-c088-aa6aa3b4c9d8"
      },
      "source": [
        "# another example\n",
        "# Paris is to France as Madrid is to what?\n",
        "############# YOUR CODE HERE ################\n",
        "Paris = nlp.vocab['Paris']\n",
        "France = nlp.vocab['France']\n",
        "Madrid = nlp.vocab['Madrid']\n",
        "\n",
        "maybe_Spain = France.vector - Paris.vector + Madrid.vector\n",
        "\n",
        "allwords = [w for w in nlp.vocab if w.has_vector and w.lower_ != 'paris' and w.lower_ != 'madrid' and w.lower_ != 'france']\n",
        "candidates = sorted(allwords, key=lambda w: cosine(maybe_Spain, w.vector), reverse=True)\n",
        "print([c.text for c in candidates[:5]])\n",
        "#############################################"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Del', 'Va', 'D.C.', 'y', 'Oct']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFPYDPuX6thm"
      },
      "source": [
        "### Sentence/Document Level Similarity\n",
        "Using word embeddings, we can also calculate similarity between sentences and documents. More advanced models such as Doc2Vec or neural networks can be used, but in this tutorial we will continue to use Word2Vec model to calculate document similarity. Since sentences and documents are composed of words, one easy way to obtain vector representations for sentences/documents is to calculate the average vectors of words. \\\\\n",
        "<br>\n",
        "Let's try to calculate the similarity among these three sentences:\n",
        "\n",
        "\n",
        "1.   Cats are beautiful animals.\n",
        "2.   Some gorgeous creatures are felines.\n",
        "3.   Dolphins are swimming mammals.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OiS9WQyG8zqG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c8ed7e2-5b17-45d1-c93c-ac81c2a452cb"
      },
      "source": [
        "# Word2Vec model does not provide vector representations for sentences \n",
        "# or documents. How is the similarity between sentences computed?\n",
        "# Since sentences are composed of words, an easy way to obtain the vector\n",
        "# representations of sentences is by averaging the vectors of each word in\n",
        "# the sentence.\n",
        "############# YOUR CODE HERE ################\n",
        "s1 = (nlp.vocab['Cats'].vector + nlp.vocab['are'].vector + nlp.vocab['beautiful'].vector + \\\n",
        "    nlp.vocab['animals'].vector + nlp.vocab['.'].vector)/5\n",
        "s2 = (nlp.vocab['Some'].vector + nlp.vocab['gorgeous'].vector + nlp.vocab['creatures'].vector + \\\n",
        "    nlp.vocab['are'].vector + nlp.vocab['felines'].vector + nlp.vocab['.'].vector)/6\n",
        "s3 = (nlp.vocab['Dolphins'].vector + nlp.vocab['are'].vector + nlp.vocab['swimming'].vector + \\\n",
        "    nlp.vocab['mammals'].vector + nlp.vocab['.'].vector)/5\n",
        "\n",
        "print(cosine(s1, s2))\n",
        "\n",
        "print(cosine(s1, s3))\n",
        "\n"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9115828\n",
            "0.7822957\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_P6Ep9F67Lu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "45c96880-c6b4-442b-a78d-7e510583c74a"
      },
      "source": [
        "# spaCy also supports similarity calculation between sentences and documents\n",
        "target = nlp(\"Cats are beautiful animals.\")  # text about cats\n",
        "\n",
        "doc1 = nlp(\"Some gorgeous creatures are felines.\")  # text about cats\n",
        "doc2 = nlp(\"Dolphins are swimming mammals.\")  # text about dolphins\n",
        "\n",
        "print('Similarity between target and doc1:', target.similarity(doc1))\n",
        "print('Similarity between target and doc1:', target.similarity(doc2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Similarity between target and doc1: 0.9115828449161616\n",
            "Similarity between target and doc1: 0.7822956256736615\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYs9_tHSFBhd"
      },
      "source": [
        "### Word Embeddings Visualization\n",
        "Since the word vectors we use have 300 dimensions, we cannot visualize them. One natural way is to apply dimension reduction first and then visualize them. We use a popular dimension reduction technique called [t-SNE](https://lvdmaaten.github.io/tsne/) (you can also use PCA) to reduce the word vectors to 2D and then plot the words in our word analogy example to see if we can find some pattern visually. \\\\\n",
        "<br>\n",
        "An interactive visualization of word embeddings can be found here: \\\\\n",
        "[https://projector.tensorflow.org/](https://projector.tensorflow.org/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUnAim1BFH50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "outputId": "e8a66496-ade2-4b1c-b0ed-a3fd2ce0bd53"
      },
      "source": [
        "# use t-SNE to do dimension reduction, from 300d to 2d\n",
        "tsne_model = TSNE(n_components=2)\n",
        "\n",
        "# get transformed vectors\n",
        "data = np.array([king.vector, man.vector, queen.vector, woman.vector])\n",
        "data_2d = tsne_model.fit_transform(data)\n",
        "\n",
        "labels = ['king', 'man', 'queen', 'woman']\n",
        "\n",
        "# plot the 2d vectors and show their labels\n",
        "plt.scatter(data_2d[:, 0], data_2d[:, 1], s=100)\n",
        "for i, txt in enumerate(labels):\n",
        "    plt.annotate(txt, (data_2d[i,0], data_2d[i,1]), xytext=(2, 3), textcoords='offset points')\n",
        "plt.show()"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXzQ0M-OW0h0"
      },
      "source": [
        "### Sentiment Analysis\n",
        "The major reason for coming up with word embedding models is that we want to use these embeddings which encode the word semantics to help us tackle problems related with natural language. \\\\\n",
        "<br>\n",
        "One such task is sentiment analysis. By analyzing the sentiment of texts, we want to understand whether a given sentence/document is positive or negative. For example, 'the weather is so nice today' has a positive sentiment whereas 'he is bored by the movie' has a negative sentiment. \\\\\n",
        "<br>\n",
        "In this tutorial, we want to use the word embeddings combined with a simple machine learning model ([logistic regression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)) to do sentiment analysis. Logistic regression is a linear classification model and in our case we want to classify whether a given sentence is positive or negative. So it's a binary classification. \\\\\n",
        "<br>\n",
        "![logistic](https://rasbt.github.io/mlxtend/user_guide/classifier/SoftmaxRegression_files/logistic_regression_schematic.png)\n",
        "<br>\n",
        "Our training data contains 2,748 from Yelp reviews, IMDB movie reviews, and Amazon reviews. In the dataset, 1 means positive and 0 means negative. The original data can be downloaded from [here](https://www.kaggle.com/rahulin05/sentiment-labelled-sentences-data-set/data), the combined file can be downloaded from [here](https://drive.google.com/file/d/1knrjvDNkiXtviXBoLm5OJY45_kcCmDxe/view?usp=sharing)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4WimFKHSCqp"
      },
      "source": [
        "### arquivo local em meusite: /content/combined_training.txt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LliIw9nW4Fc",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "outputId": "fccb00e7-6c1a-4d09-eb78-b7f1207a31b0"
      },
      "source": [
        "# load files into the environment\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e87f3eb7-25a3-4069-a27f-f64840a448ad\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-e87f3eb7-25a3-4069-a27f-f64840a448ad\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-6e49a2f33e1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# load files into the environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0muploaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m   result = _output.eval_js(\n\u001b[1;32m     63\u001b[0m       'google.colab._files._uploadFiles(\"{input_id}\", \"{output_id}\")'.format(\n\u001b[0;32m---> 64\u001b[0;31m           input_id=input_id, output_id=output_id))\n\u001b[0m\u001b[1;32m     65\u001b[0m   \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_collections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_six\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     if (reply.get('type') == 'colab_reply' and\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGWb5845wEZZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a96210fc-ca2a-45be-be81-c0453c2f0b21"
      },
      "source": [
        "# read data\n",
        "data_raw = []\n",
        "with open('/content/combined_training.txt', newline='') as fr:\n",
        "    reader = csv.reader(fr, delimiter='\\t')\n",
        "    for row in reader:\n",
        "        data_raw.append([row[0], int(row[1])])\n",
        "\n",
        "# print the number of data\n",
        "print(len(data_raw))\n",
        "\n",
        "# print the last data item\n",
        "print(data_raw[-1])"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2748\n",
            "[\"Then, as if I hadn't wasted enough of my life there, they poured salt in the wound by drawing out the time it took to bring the check.\", 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8U9GGTwFzTnE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fe5ab05-c61f-4819-d8df-991a72123cad"
      },
      "source": [
        "x_train = np.array([nlp(d[0]).vector for d in data_raw])\n",
        "y_train = np.array([d[1] for d in data_raw])\n",
        "\n",
        "# print the dimension of x\n",
        "print(x_train.shape)\n",
        "\n",
        "# print the dimension of y\n",
        "print(y_train.shape)\n",
        "\n",
        "# double check\n",
        "print(nlp(data_raw[-1][0]).text, y_train[-1])"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2748, 300)\n",
            "(2748,)\n",
            "Then, as if I hadn't wasted enough of my life there, they poured salt in the wound by drawing out the time it took to bring the check. 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwSJnk0A15LB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53624c0c-3e53-4655-8dc7-3422736d062f"
      },
      "source": [
        "logreg = linear_model.LogisticRegression()\n",
        "logreg.fit(x_train, y_train)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTXDcQRq2JHF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cbc7426-c070-4c02-f624-f748f493a16b"
      },
      "source": [
        "# predict using trained model\n",
        "predict = logreg.predict(np.array([nlp('the weather today is pleasant').vector, nlp('the food in this restaurant is beyond my expectation').vector]))\n",
        "print(predict)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 0]\n"
          ]
        }
      ]
    }
  ]
}